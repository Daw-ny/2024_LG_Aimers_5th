{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a48e6e",
   "metadata": {},
   "source": [
    "# 불량품 예측\n",
    "\n",
    "불량품을 예측하기 위해 다음과 같은 함수화 정리를 진행한다. 혼란을 막기 위해 모든 과정을 함수화 하기로 한다.  \n",
    "목차는 다음과 같다.\n",
    "\n",
    "- 1. Load packages & Data\n",
    "- 2. Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab477ae",
   "metadata": {},
   "source": [
    "## 1. Load Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e5c4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### ide packages\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    recall_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier, plot_metric\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea357c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "load_dir = './data/'\n",
    "train = pd.read_csv(load_dir + \"train.csv\")\n",
    "test = pd.read_csv(load_dir + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d3439",
   "metadata": {},
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195e730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 스코어 지정하기\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1, average = 'binary')\n",
    "\n",
    "# 평가 매트릭 계산 결과 보여주기\n",
    "def get_clf_eval(y_test, y_pred=None):\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
    "\n",
    "    print(\"오차행렬:\\n\", confusion)\n",
    "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
    "    print(\"정밀도: {:.4f}\".format(precision))\n",
    "    print(\"재현율: {:.4f}\".format(recall))\n",
    "    print(\"F1: {:.4f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1867a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 공정 맞춤형 위치 옮기기\n",
    "def move_data(data):\n",
    "    # divide\n",
    "    dam = data.filter(regex='_Dam')\n",
    "    fill1 = data.filter(regex='_Fill1')\n",
    "    fill2 = data.filter(regex='_Fill2')\n",
    "    autoclave = data.filter(regex='_AutoClave')\n",
    "    target = data['target']\n",
    "\n",
    "    # dam\n",
    "    dam = dam.dropna(axis=1, how='all')\n",
    "    dam = dam.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam')\n",
    "    dam_mask = dam[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan])].iloc[:, 24:].shift(-1, axis = 1).values\n",
    "    dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[24:]] = dam_mask\n",
    "    dam = dam.drop(columns='WorkMode Collect Result_Dam')\n",
    "\n",
    "    # fill1\n",
    "    fill1 = fill1.dropna(axis=1, how='all')\n",
    "    fill1 = fill1.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1')\n",
    "    fill1_mask = fill1[fill1['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan])].iloc[:, 14:].shift(-1, axis = 1).values\n",
    "    fill1.loc[fill1['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan]), fill1.columns[14:]] = fill1_mask\n",
    "    fill1 = fill1.drop(columns='WorkMode Collect Result_Fill1')\n",
    "\n",
    "    # fill2\n",
    "    fill2 = fill2.dropna(axis=1, how='all')\n",
    "    fill2 = fill2.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2')\n",
    "    fill2_mask = fill2[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan])].iloc[:, 24:].shift(-1, axis = 1).values\n",
    "    fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[24:]] = fill2_mask\n",
    "    fill2 = fill2.drop(columns='WorkMode Collect Result_Fill2')\n",
    "\n",
    "    # CONCAT\n",
    "    data = pd.concat([dam, fill1, fill2, autoclave, target], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a166392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 칼럼 위치 변경\n",
    "def swap_columns(df, condition, col1, col2):\n",
    "    # 조건에 해당하는 행 필터링\n",
    "    filtered_df = df[condition]\n",
    "    \n",
    "    # 값 교환\n",
    "    df.loc[condition, [col1, col2]] = filtered_df[[col1, col2]].copy().iloc[:, ::-1].values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8efeeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment 방향 통일과 잘못된 위치 재조정하기\n",
    "def change_data(train, test):\n",
    "    # train\n",
    "    train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] = train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "    train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] = train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float)\n",
    "\n",
    "\n",
    "    # 이동 전\n",
    "    X_sum_down_1 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "    X_sum_down_2 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "    X_sum_up_1 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "    X_sum_up_2 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "    X_sum_down = (X_sum_down_1 - X_sum_down_2) / 2 # stage1에서 빼고, Stage3에서 더하기 <500\n",
    "    X_sum_up = (X_sum_up_2 - X_sum_up_1) / 2 # stage1에서 더하고, Stage 3에서 빼기\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] += X_sum_up\n",
    "    train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] -= X_sum_up\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] -= X_sum_down\n",
    "    train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] += X_sum_down\n",
    "\n",
    "    # test\n",
    "    test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] = test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "    test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] = test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float)\n",
    "\n",
    "\n",
    "    # 이동 전\n",
    "    test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] += X_sum_up\n",
    "    test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] -= X_sum_up\n",
    "\n",
    "    test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] -= X_sum_down\n",
    "    test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] += X_sum_down\n",
    "\n",
    "    # train\n",
    "    Y_sum_dam_1 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "    Y_sum_dam_2 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] = Y_sum_dam_1 + Y_sum_dam_2 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam']\n",
    "\n",
    "    Y_sum_dam_3 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "    Y_sum_dam_4 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'] = Y_sum_dam_3 + Y_sum_dam_4 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "    Y_sum_dam_5 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float).mean()\n",
    "    Y_sum_dam_6 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] = Y_sum_dam_5 + Y_sum_dam_6 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "\n",
    "    Y_sum_fill_1 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float).mean()\n",
    "    Y_sum_fill_2 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float).mean()\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] = Y_sum_fill_1 + Y_sum_fill_2 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1']\n",
    "\n",
    "    Y_sum_fill_3 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float).mean()\n",
    "    Y_sum_fill_4 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float).mean()\n",
    "\n",
    "    train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'] = Y_sum_fill_3 + Y_sum_fill_4 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "    Y_sum_fill_5 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float).mean()\n",
    "    Y_sum_fill_6 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float).mean()\n",
    "    train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] = Y_sum_fill_5 + Y_sum_fill_6 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "\n",
    "    # test\n",
    "    test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] = Y_sum_dam_1 + Y_sum_dam_2 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam']\n",
    "    test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] = Y_sum_dam_5 + Y_sum_dam_6 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "    test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'] = Y_sum_dam_3 + Y_sum_dam_4 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "    test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] = Y_sum_fill_1 + Y_sum_fill_2 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1']\n",
    "    test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] = Y_sum_fill_5 + Y_sum_fill_6 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "    test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'] = Y_sum_fill_3 + Y_sum_fill_4 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "    ### Train\n",
    "    # 조건을 만족하는 행 인덱스를 찾음\n",
    "    condition = train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) >= 200\n",
    "\n",
    "    # DISCHARGED TIME OF RESIN(Stage1) \n",
    "    swap_columns(train, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # Dispense Volume(Stage1)\n",
    "    swap_columns(train, condition, 'Dispense Volume(Stage1) Collect Result_Dam', 'Dispense Volume(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # Stage1 Circle1 Distance Speed Collect\n",
    "    swap_columns(train, condition, 'Stage1 Circle1 Distance Speed Collect Result_Dam', 'Stage3 Circle1 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(train, condition, 'Stage1 Circle2 Distance Speed Collect Result_Dam', 'Stage3 Circle2 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(train, condition, 'Stage1 Circle3 Distance Speed Collect Result_Dam', 'Stage3 Circle3 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(train, condition, 'Stage1 Circle4 Distance Speed Collect Result_Dam', 'Stage3 Circle4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "    # Stage1 Line1 Distance Speed Collect\n",
    "    swap_columns(train, condition, 'Stage1 Line1 Distance Speed Collect Result_Dam', 'Stage3 Line1 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(train, condition, 'Stage1 Line2 Distance Speed Collect Result_Dam', 'Stage3 Line2 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(train, condition, 'Stage1 Line3 Distance Speed Collect Result_Dam', 'Stage3 Line3 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(train, condition, 'Stage1 Line4 Distance Speed Collect Result_Dam', 'Stage3 Line4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "    # THICKNESS 1\n",
    "    # swap_columns(train, condition, 'THICKNESS 1 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam')\n",
    "\n",
    "    ### 젤 마지막에 와야됨!!!!\n",
    "    # HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "\n",
    "    ### Test\n",
    "    # 조건을 만족하는 행 인덱스를 찾음\n",
    "    condition = test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) >= 200\n",
    "\n",
    "    # DISCHARGED TIME OF RESIN(Stage1) \n",
    "    swap_columns(test, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # Dispense Volume(Stage1)\n",
    "    swap_columns(test, condition, 'Dispense Volume(Stage1) Collect Result_Dam', 'Dispense Volume(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "    # Stage1 Circle1 Distance Speed Collect\n",
    "    swap_columns(test, condition, 'Stage1 Circle1 Distance Speed Collect Result_Dam', 'Stage3 Circle1 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(test, condition, 'Stage1 Circle2 Distance Speed Collect Result_Dam', 'Stage3 Circle2 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(test, condition, 'Stage1 Circle3 Distance Speed Collect Result_Dam', 'Stage3 Circle3 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(test, condition, 'Stage1 Circle4 Distance Speed Collect Result_Dam', 'Stage3 Circle4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "    # Stage1 Line1 Distance Speed Collect\n",
    "    swap_columns(test, condition, 'Stage1 Line1 Distance Speed Collect Result_Dam', 'Stage3 Line1 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(test, condition, 'Stage1 Line2 Distance Speed Collect Result_Dam', 'Stage3 Line2 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(test, condition, 'Stage1 Line3 Distance Speed Collect Result_Dam', 'Stage3 Line3 Distance Speed Collect Result_Dam')\n",
    "    swap_columns(test, condition, 'Stage1 Line4 Distance Speed Collect Result_Dam', 'Stage3 Line4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "    # THICKNESS 1\n",
    "    # swap_columns(train, condition, 'THICKNESS 1 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam')\n",
    "\n",
    "    ### 젤 마지막에 와야됨!!!!\n",
    "    # HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam')\n",
    "    \n",
    "    ### Train\n",
    "    # 조건을 만족하는 행 인덱스를 찾음\n",
    "    condition = train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float) < 200\n",
    "\n",
    "    # DISCHARGED TIME OF RESIN(Stage1)\n",
    "    swap_columns(train, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # Dispense Volume(Stage1)\n",
    "    swap_columns(train, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # 반드시 마지막으로 와야함!!!!\n",
    "    # HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    ### Test\n",
    "    condition = test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float) < 200\n",
    "\n",
    "    # DISCHARGED TIME OF RESIN(Stage1)\n",
    "    swap_columns(test, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # Dispense Volume(Stage1)\n",
    "    swap_columns(test, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "    # 반드시 마지막으로 와야함!!!!\n",
    "    # HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1')\n",
    "    \n",
    "    ### Train\n",
    "    condition = train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500\n",
    "\n",
    "    # DISCHARGED TIME OF RESIN(Stage1)\n",
    "    swap_columns(train, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # Dispense Volume(Stage1)\n",
    "    swap_columns(train, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # 반드시 마지막으로 와야함!!!!\n",
    "    # HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "    swap_columns(train, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    ### Test\n",
    "    condition = test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500\n",
    "\n",
    "    # DISCHARGED TIME OF RESIN(Stage1)\n",
    "    swap_columns(test, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # Dispense Volume(Stage1)\n",
    "    swap_columns(test, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    # 반드시 마지막으로 와야함!!!!\n",
    "    # HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "    swap_columns(test, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32fad315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dam, Fill1, Fill2에서 지정된 값이 다를 경우 Abnormal \n",
    "def inconsistant(data, columnname, iwantthiscolumnsname, is_train = True):\n",
    "    # 장비 번호가 다르면 불일치\n",
    "    if is_train:\n",
    "        cri = [\n",
    "            df_train[columnname + '_Dam'] != df_train[columnname + '_Fill1'],\n",
    "            df_train[columnname + '_Dam'] != df_train[columnname + '_Fill2'],\n",
    "            df_train[columnname + '_Fill1'] != df_train[columnname + '_Fill2'],\n",
    "            data[iwantthiscolumnsname] == 1\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        cri = [\n",
    "            df_test[columnname + '_Dam'] != df_test[columnname + '_Fill1'],\n",
    "            df_test[columnname + '_Dam'] != df_test[columnname + '_Fill2'],\n",
    "            df_test[columnname + '_Fill1'] != df_test[columnname + '_Fill1'],\n",
    "            data[iwantthiscolumnsname] == 1\n",
    "        ]\n",
    "    con = [1, 1, 1, 1]\n",
    "\n",
    "    data[iwantthiscolumnsname] = np.select(cri, con, default = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804698ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 세팅\n",
    "def variable_setting(types, tr, te, columns_to_convert, columns_to):\n",
    "    train = tr.copy()\n",
    "    test = te.copy()\n",
    "    \n",
    "    if types == 'catboost':\n",
    "        dtype = 'string'  # 원하는 데이터 타입\n",
    "        for column in columns_to_convert + columns_to:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "\n",
    "        dtype = 'category'  # 원하는 데이터 타입\n",
    "        for column in columns_to_convert + columns_to:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "    elif types == 'lightgbm':\n",
    "        dtype = 'float'  # 원하는 데이터 타입\n",
    "        for column in columns_to_convert + columns_to:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "\n",
    "        dtype = 'category'  # 원하는 데이터 타입\n",
    "        for column in columns_to_convert + columns_to:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "    elif types == 'xgboost':\n",
    "        dtype = 'float'  # 원하는 데이터 타입\n",
    "        for column in columns_to_convert + columns_to:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "        dtype = 'category'  # 원하는 데이터 타입\n",
    "        for column in columns_to_convert + columns_to:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc05ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_best_threshold(model, X_valid, y_valid):\n",
    "    # Precision - Recall\n",
    "    y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_proba)\n",
    "    f1_scores = 2*recall*precision / (recall + precision)\n",
    "    cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    y_pred_custom_threshold = (y_pred_proba >= cat_best_threshold).astype(int)\n",
    "    \n",
    "    return thresholds, y_pred_custom_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bee5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_optuna(train, cat_features_indices):\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = XGBClassifier(eval_metric='logloss', **params, early_stopping_rounds = 50, enable_categorical=True)\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 평가\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        # thresholds\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid_cat_last, cat_proba)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold_cat = (cat_proba >= cat_best_threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred_custom_threshold_cat)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    # Optuna 스터디 생성 및 최적화\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    return study.best_trial.params, X, y, X_train.index, X_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f131662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_optuna(train, cat_features_indices):\n",
    "\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    def objective(trial):\n",
    "        lgbm_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 400, 1500),\n",
    "            \"max_depth\": trial.suggest_int('max_depth', 3, 63),\n",
    "            \"learning_rate\": trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True), \n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "            \"min_child_weight\": trial.suggest_float('min_child_weight', 0.5, 4),\n",
    "            \"min_child_samples\": trial.suggest_int('min_child_samples', 5, 100),\n",
    "            \"subsample\": trial.suggest_float('subsample', 0.4, 1),\n",
    "            \"subsample_freq\": trial.suggest_int('subsample_freq', 0, 5),\n",
    "            \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.2, 1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 64),\n",
    "        }\n",
    "\n",
    "        model = LGBMClassifier(**lgbm_params, device='cpu', random_state=42, verbose=-1)\n",
    "\n",
    "        # 범주형 피처 적용\n",
    "        model.fit(X_train, y_train, categorical_feature=cat_features_indices,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 평가\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        # thresholds\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid_cat_last, cat_proba)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold_cat = (cat_proba >= cat_best_threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred_custom_threshold_cat)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    # Optuna 스터디 생성 및 최적화\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    return study.best_trial.params, X, y, X_train.index, X_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c3b707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_optuna(train, cat_features_indices):\n",
    "    \n",
    "    # train X, y\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "\n",
    "    # $plit \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Pooling\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features_indices)\n",
    "    valid_pool = Pool(X_valid, y_valid, cat_features=cat_features_indices)\n",
    "    \n",
    "    # tuning parameters\n",
    "    def objective(trial):\n",
    "        # 하이퍼파라미터를 샘플링\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 1e-9, 10.0),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "            \"od_type\": trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "            \"od_wait\": trial.suggest_int(\"od_wait\", 10, 50),\n",
    "            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "#             \"scale_pos_weight\": trial.suggest_int('scale_pos_weight', 6, 10),\n",
    "            \"verbose\": 0,\n",
    "            \"random_seed\": 42\n",
    "        }\n",
    "\n",
    "        # CatBoost 모델 학습\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50, verbose=0)\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 평가\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        # thresholds\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid_cat_last, cat_proba)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold_cat = (cat_proba >= cat_best_threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred_custom_threshold_cat)\n",
    "        \n",
    "        return f1\n",
    "\n",
    "    # Optuna 스터디 생성 및 최적화\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    return study.best_trial.params, X, y, X_train.index, X_valid.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443902e",
   "metadata": {},
   "source": [
    "## 3. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae0c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기준\n",
    "columnname = ['Equipment', 'Receip No Collect Result', 'Production Qty Collect Result', 'PalletID Collect Result', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4faaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop oolumns\n",
    "drop_col = [\n",
    "    'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle4 Distance Speed Collect Result_Dam', \n",
    "    'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle4 Distance Speed Collect Result_Dam', \n",
    "    'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    # Dam, Fill2의 경우 Z값이 서로 같다. -> 그렇다면 Fill1은 높이값에서 흔들린 경우가 있다는 것을 의미한다.\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2',\n",
    "    \n",
    "    # 의미를 찾을 수 없는 컬럼들 제거\n",
    "    'Wip Line_Fill1', \n",
    "    'Process Desc._Fill1', \n",
    "    'Insp. Seq No._Fill1', \n",
    "    'Insp Judge Code_Fill1', \n",
    "    'Equipment_AutoClave',\n",
    "    'Process Desc._AutoClave', \n",
    "    'Wip Line_AutoClave', \n",
    "    'Insp Judge Code_AutoClave',\n",
    "    'Insp. Seq No._AutoClave',\n",
    "    '1st Pressure Judge Value_AutoClave', \n",
    "    '2nd Pressure Judge Value_AutoClave', \n",
    "    '3rd Pressure Judge Value_AutoClave', \n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Unit Time_AutoClave',\n",
    "    'Wip Line_Fill2', \n",
    "    'Process Desc._Fill2', \n",
    "    'Insp. Seq No._Fill2', \n",
    "    'Insp Judge Code_Fill2', \n",
    "    'Wip Line_Dam', \n",
    "    'Process Desc._Dam', \n",
    "    'Insp. Seq No._Dam', \n",
    "    'Insp Judge Code_Dam',\n",
    "    'CURE END POSITION X Collect Result_Dam',\n",
    "    'CURE END POSITION Z Collect Result_Dam',\n",
    "    'CURE END POSITION Θ Collect Result_Dam',\n",
    "    'CURE STANDBY POSITION X Collect Result_Dam',\n",
    "    'CURE STANDBY POSITION Z Collect Result_Dam',\n",
    "    'CURE STANDBY POSITION Θ Collect Result_Dam',\n",
    "    \n",
    "    # Fill2는 레진을 살포하지 않는다. UV만 진행하는 과정이므로 싹 삭제해 준다.          \n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2',\n",
    "    'HEAD Standby Position X Collect Result_Fill2',\n",
    "    'HEAD Standby Position Y Collect Result_Fill2',\n",
    "    'HEAD Standby Position Z Collect Result_Fill2',\n",
    "    'Head Clean Position X Collect Result_Fill2',\n",
    "    'Head Clean Position Y Collect Result_Fill2',\n",
    "    'Head Clean Position Z Collect Result_Fill2',\n",
    "    'Head Purge Position X Collect Result_Fill2',\n",
    "    'Head Purge Position Y Collect Result_Fill2',\n",
    "    'Head Purge Position Z Collect Result_Fill2',\n",
    "    'DISCHARGED SPEED OF RESIN Collect Result_Fill2',\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill2',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill2',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill2',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill2',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill2',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill2',\n",
    "    \n",
    "    'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    # 단일값이 하나인 컬럼들, 의미를 찾고싶다면 주석처리 해야하는 것들\n",
    "    'CURE START POSITION X Collect Result_Dam', # Equipment에 따라서 정해지며, 하나로 책정됨.\n",
    "    'CURE START POSITION Z Collect Result_Dam', # START POSITION\n",
    "    'CURE START POSITION Θ Collect Result_Dam', # Equipment에 따라서 정해지며, 하나로 책정됨.\n",
    "    'HEAD Standby Position X Collect Result_Dam',\n",
    "    'HEAD Standby Position Y Collect Result_Dam',\n",
    "    'HEAD Standby Position Z Collect Result_Dam',\n",
    "    'Head Clean Position X Collect Result_Dam',\n",
    "    'Head Clean Position Y Collect Result_Dam', # 흔들림에 따라 Z\n",
    "    'Head Purge Position X Collect Result_Dam',\n",
    "    'Head Purge Position Y Collect Result_Dam',\n",
    "    'Head Zero Position X Collect Result_Dam',\n",
    "    'HEAD Standby Position X Collect Result_Fill1',\n",
    "    'HEAD Standby Position Y Collect Result_Fill1',\n",
    "    'HEAD Standby Position Z Collect Result_Fill1',\n",
    "    'Head Clean Position X Collect Result_Fill1',\n",
    "    'Head Clean Position Y Collect Result_Fill1',\n",
    "    'Head Clean Position Z Collect Result_Fill1',\n",
    "    'Head Purge Position X Collect Result_Fill1',\n",
    "    'Head Purge Position Y Collect Result_Fill1',\n",
    "    'CURE END POSITION X Collect Result_Fill2',\n",
    "    'CURE END POSITION Θ Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION X Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Θ Collect Result_Fill2',\n",
    "    'CURE START POSITION X Collect Result_Fill2',\n",
    "    'CURE START POSITION Θ Collect Result_Fill2',\n",
    "    \n",
    "    # AutoClave 의미없어보이는거 제거\n",
    "    'Chamber Temp. Judge Value_AutoClave',\n",
    "#     'Chamber Temp. Unit Time_AutoClave',\n",
    "#     '1st Pressure_AutoClave',\n",
    "#     '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "#     '2nd Pressure_AutoClave',\n",
    "#     '2nd Pressure Unit Time_AutoClave',\n",
    "#     '3rd Pressure_AutoClave',\n",
    "#     '3rd Pressure Unit Time_AutoClave',\n",
    "    'Chamber Temp. Collect Result_AutoClave',\n",
    "    '1st Pressure Collect Result_AutoClave',\n",
    "    '2nd Pressure Collect Result_AutoClave',\n",
    "    '3rd Pressure Collect Result_AutoClave',\n",
    "#     'rount_1st_time', 'rount_2nd_time', 'rount_3rd_time',\n",
    "    'all_time',\n",
    "    \n",
    "\n",
    "    \n",
    "    # Model.Suffix, Workorder이 같다.\n",
    "    'Model.Suffix_Fill1', 'Model.Suffix_Fill2', 'Model.Suffix_AutoClave',\n",
    "    'Workorder_Fill1', 'Workorder_Fill2', 'Workorder_AutoClave',\n",
    "    \n",
    "    # 아무 의미 없는 값\n",
    "    'Chamber Temp. Judge Value_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Unit Time_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave',\n",
    "    \n",
    "    # k-means 생성 피처\n",
    "    'cure_x_dist_dam',\n",
    "    'cure_z_dist_dam', 'cure_z_dist_fill2',\n",
    "    \n",
    "    # 중복 데이터 없애기\n",
    "#     'PalletID Collect Result_Fill1', \n",
    "#     'Production Qty Collect Result_Fill1',\n",
    "#     'Receip No Collect Result_Fill1',\n",
    "#     'PalletID Collect Result_Fill2', \n",
    "#     'Production Qty Collect Result_Fill2',\n",
    "#     'Receip No Collect Result_Fill2',\n",
    "#     'Equipment_Fill1',\n",
    "#     'Equipment_Fill2'\n",
    "    \n",
    "    \n",
    "#     'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    "#     'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "#     'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    "#     'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    "#     'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "#     'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    "#     'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    "#     'Head Purge Position Z Collect Result_Fill1',\n",
    "#     'Machine Tact time Collect Result_Fill1',\n",
    "    'Production Qty Collect Result_Fill1',\n",
    "    'Production Qty Collect Result_Fill2',\n",
    "#     'Minus1_Fill1', 'Minus2_Fill1',\n",
    "#     'Minus1Y_Fill1', 'Minus2Y_Fill1',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2794d",
   "metadata": {},
   "source": [
    "## 4. Matched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12dda308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73/691509256.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.     0.012  0.    ...  0.    -0.019  0.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[24:]] = dam_mask\n",
      "/tmp/ipykernel_73/691509256.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[114.612 114.612  85.    ...  85.    114.612  85.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[24:]] = fill2_mask\n",
      "/tmp/ipykernel_73/691509256.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.054  0.     0.    ...  0.     0.     0.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[24:]] = dam_mask\n",
      "/tmp/ipykernel_73/691509256.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[85. 85. 85. ... 85. 85. 85.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[24:]] = fill2_mask\n"
     ]
    }
   ],
   "source": [
    "# 위치 옮기기\n",
    "train_move = move_data(train)\n",
    "test_move = move_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80261221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 변경 및 Equipment2 를 Equipment1 방향으로 변경\n",
    "df_train, df_test = change_data(train_move, test_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c8dbe",
   "metadata": {},
   "source": [
    "### Type Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18ac0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 변경하기\n",
    "type_change = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1']\n",
    "\n",
    "for i in type_change:\n",
    "    df_train[i] = df_train[i].astype('float64')\n",
    "    df_test[i] = df_test[i].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12049aa2",
   "metadata": {},
   "source": [
    "### Modified Equipment data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0314fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment 번호만 가져오기\n",
    "df_train['Equipment_Dam'] = df_train['Equipment_Dam'].str.slice(15, 16)\n",
    "df_train['Equipment_Fill1'] = df_train['Equipment_Fill1'].str.slice(17, 18)\n",
    "df_train['Equipment_Fill2'] = df_train['Equipment_Fill2'].str.slice(17, 18)\n",
    "\n",
    "df_test['Equipment_Dam'] = df_test['Equipment_Dam'].str.slice(15, 16)\n",
    "df_test['Equipment_Fill1'] = df_test['Equipment_Fill1'].str.slice(17, 18)\n",
    "df_test['Equipment_Fill2'] = df_test['Equipment_Fill2'].str.slice(17, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc05aa",
   "metadata": {},
   "source": [
    "### New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712525b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불일치 변수\n",
    "df_train['inconsistant'] = 0\n",
    "df_test['inconsistant'] = 0\n",
    "\n",
    "# 장착\n",
    "for i in columnname:\n",
    "    inconsistant(df_train, i, 'inconsistant', True)\n",
    "    inconsistant(df_test, i, 'inconsistant', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ed4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 0이하, 900이상인 값은 이상치로 분류\n",
    "for j in ['Machine Tact time Collect Result_Dam', 'Machine Tact time Collect Result_Fill1', 'Machine Tact time Collect Result_Fill2']:\n",
    "    cri = [\n",
    "        df_train[j] <= 0,\n",
    "        df_train[j] > 900\n",
    "    ]\n",
    "    cri2 = [\n",
    "        df_test[j] <= 0,\n",
    "        df_test[j] > 900\n",
    "    ]\n",
    "    con = [\n",
    "        1, 1\n",
    "    ]\n",
    "    df_train['inconsistant'] = np.select(cri, con, default = df_train['inconsistant'])\n",
    "    df_test['inconsistant'] = np.select(cri2, con, default = df_test['inconsistant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad0716",
   "metadata": {},
   "source": [
    "### Speed Line & Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b63870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라인별로 속도가 같아야 정상이다.\n",
    "df_train['Stage1 Line diffent Distance Speed_Dam'] = ((df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_train['Stage1 Line Sum Speed_Dam'] = df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_train['Stage2 Line diffent Distance Speed_Dam'] = ((df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_train['Stage2 Line Sum Speed_Dam'] = df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_train['Stage3 Line diffent Distance Speed_Dam'] = ((df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_train['Stage3 Line Sum Speed_Dam'] = df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line4 Distance Speed Collect Result_Dam']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라인별로 속도가 같아야 정상이다.\n",
    "df_test['Stage1 Line diffent Distance Speed_Dam'] = ((df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_test['Stage1 Line Sum Speed_Dam'] = df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['Stage2 Line diffent Distance Speed_Dam'] = ((df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_test['Stage2 Line Sum Speed_Dam'] = df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['Stage3 Line diffent Distance Speed_Dam'] = ((df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_test['Stage3 Line Sum Speed_Dam'] = df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line4 Distance Speed Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ffa91",
   "metadata": {},
   "source": [
    "### Cure differenciates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8fe9f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 경화 x 좌표 dam\n",
    "df_train['cure_x_dist_dam'] = df_train['CURE START POSITION X Collect Result_Dam'] - df_train['CURE END POSITION X Collect Result_Dam']\n",
    "df_test['cure_x_dist_dam'] = df_test['CURE START POSITION X Collect Result_Dam'] - df_test['CURE END POSITION X Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16671d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경화 z좌표 dam\n",
    "df_train['cure_z_dist_dam'] = df_train['CURE START POSITION Z Collect Result_Dam'] - df_train['CURE END POSITION Z Collect Result_Dam']\n",
    "df_test['cure_z_dist_dam'] = df_test['CURE START POSITION Z Collect Result_Dam'] - df_test['CURE END POSITION Z Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae33f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경화 z좌표 fill2\n",
    "df_train['cure_z_dist_fill2'] = df_train['CURE START POSITION Z Collect Result_Fill2'] - df_train['CURE END POSITION Z Collect Result_Fill2']\n",
    "df_test['cure_z_dist_fill2'] = df_test['CURE START POSITION Z Collect Result_Fill2'] - df_test['CURE END POSITION Z Collect Result_Fill2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a9884",
   "metadata": {},
   "source": [
    "### time 보정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a2bfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time 보정하기\n",
    "df_train['rount_1st_time'] = round(df_train['1st Pressure 1st Pressure Unit Time_AutoClave'], -1)\n",
    "df_train['rount_2nd_time'] = round(df_train['2nd Pressure Unit Time_AutoClave'], -1)\n",
    "df_train['rount_3rd_time'] = round(df_train['3rd Pressure Unit Time_AutoClave'], -1)\n",
    "df_train['all_time'] = round(df_train['Chamber Temp. Unit Time_AutoClave'], -1)\n",
    "\n",
    "df_test['rount_1st_time'] = round(df_test['1st Pressure 1st Pressure Unit Time_AutoClave'], -1)\n",
    "df_test['rount_2nd_time'] = round(df_test['2nd Pressure Unit Time_AutoClave'], -1)\n",
    "df_test['rount_3rd_time'] = round(df_test['3rd Pressure Unit Time_AutoClave'], -1)\n",
    "df_test['all_time'] = round(df_test['Chamber Temp. Unit Time_AutoClave'], -1)\n",
    "\n",
    "time_col = [\n",
    "    '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "    '2nd Pressure Unit Time_AutoClave',\n",
    "    '3rd Pressure Unit Time_AutoClave',\n",
    "    'Chamber Temp. Unit Time_AutoClave'\n",
    "]\n",
    "\n",
    "# 적용\n",
    "df_train = df_train.drop(columns = time_col, axis = 1)\n",
    "df_test = df_test.drop(columns = time_col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a998f9e",
   "metadata": {},
   "source": [
    "### 단일화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4844819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receip 단일화\n",
    "dtype = 'string'  # 원하는 데이터 타입\n",
    "for column in ['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2']:\n",
    "    df_train[column] = df_train[column].astype(dtype)\n",
    "    df_test[column] = df_test[column].astype(dtype)\n",
    "\n",
    "df_train['Receip No'] = df_train['Receip No Collect Result_Dam'] + df_train['Receip No Collect Result_Fill1'] + df_train['Receip No Collect Result_Fill2']\n",
    "df_test['Receip No'] = df_test['Receip No Collect Result_Dam'] + df_test['Receip No Collect Result_Fill1'] + df_test['Receip No Collect Result_Fill2']\n",
    "\n",
    "df_train = df_train.drop(columns = ['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])\n",
    "df_test = df_test.drop(columns = ['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65a33511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment와 PalletID 하나로 만들기\n",
    "df_train['Equipment'] = df_train['Equipment_Dam'] + df_train['Equipment_Fill1'] + df_train['Equipment_Fill2']\n",
    "df_test['Equipment'] = df_test['Equipment_Dam'] + df_test['Equipment_Fill1'] + df_test['Equipment_Fill2']\n",
    "\n",
    "df_train = df_train.drop(columns = ['Equipment_Dam', 'Equipment_Fill1', 'Equipment_Fill2'])\n",
    "df_test = df_test.drop(columns = ['Equipment_Dam', 'Equipment_Fill1', 'Equipment_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "279315b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PalletID 단일화\n",
    "dtype = 'int'  # 원하는 데이터 타입\n",
    "for column in ['PalletID Collect Result_Dam', 'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2']:\n",
    "    df_train[column] = df_train[column].astype(dtype)\n",
    "    df_test[column] = df_test[column].astype(dtype)\n",
    "    \n",
    "dtype = 'string'  # 원하는 데이터 타입\n",
    "for column in ['PalletID Collect Result_Dam', 'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2']:\n",
    "    df_train[column] = df_train[column].astype(dtype)\n",
    "    df_test[column] = df_test[column].astype(dtype)\n",
    "    \n",
    "df_train['PalletID'] = df_train['PalletID Collect Result_Dam'] + df_train['PalletID Collect Result_Fill1']\n",
    "df_test['PalletID'] = df_test['PalletID Collect Result_Dam'] + df_test['PalletID Collect Result_Fill1']\n",
    "\n",
    "df_train = df_train.drop(columns = ['PalletID Collect Result_Dam', 'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2'])\n",
    "df_test = df_test.drop(columns = ['PalletID Collect Result_Dam', 'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0912a8",
   "metadata": {},
   "source": [
    "### 각 요인별 곱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1103566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['1st Pressure x Time x Temp AutoClave'] = df_train['1st Pressure Collect Result_AutoClave']*df_train['rount_1st_time']*df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "df_train['2nd Pressure x Time x Temp AutoClave'] = df_train['2nd Pressure Collect Result_AutoClave']*df_train['rount_2nd_time']*df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "df_train['3rd Pressure x Time x Temp AutoClave'] = df_train['3rd Pressure Collect Result_AutoClave']*df_train['rount_3rd_time']*df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "\n",
    "df_test['1st Pressure x Time x Temp AutoClave'] = df_test['1st Pressure Collect Result_AutoClave']*df_test['rount_1st_time']*df_test['Chamber Temp. Collect Result_AutoClave']\n",
    "df_test['2nd Pressure x Time x Temp AutoClave'] = df_test['2nd Pressure Collect Result_AutoClave']*df_test['rount_2nd_time']*df_test['Chamber Temp. Collect Result_AutoClave']\n",
    "df_test['3rd Pressure x Time x Temp AutoClave'] = df_test['3rd Pressure Collect Result_AutoClave']*df_test['rount_3rd_time']*df_test['Chamber Temp. Collect Result_AutoClave']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad827a",
   "metadata": {},
   "source": [
    "### 각 좌표별 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fe3f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Minus1_Dam']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Minus2_Dam']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Minus1_Dam']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Minus2_Dam']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Minus1_Fill1']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1']\n",
    "df_train['Minus2_Fill1']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_test['Minus1_Fill1']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1']\n",
    "df_test['Minus2_Fill1']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['Minus1Y_Dam']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Minus2Y_Dam']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Minus1Y_Dam']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Minus2Y_Dam']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Minus1Y_Fill1']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "df_train['Minus2Y_Fill1']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_test['Minus1Y_Fill1']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "df_test['Minus2Y_Fill1']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['Minus1Y_Dam'] = df_train['Minus1Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "df_train['Minus2Y_Dam'] = df_train['Minus2Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "\n",
    "df_test['Minus1Y_Dam'] = df_test['Minus1Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "df_test['Minus2Y_Dam'] = df_test['Minus2Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeb695",
   "metadata": {},
   "source": [
    "### 타입 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c767600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834658bc",
   "metadata": {},
   "source": [
    "### K-means cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7ca9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_col = [\n",
    "    'CURE SPEED Collect Result_Dam',\n",
    "    'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage1) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage2) Collect Result_Dam',\n",
    "    'Dispense Volume(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
    "    'Head Clean Position Z Collect Result_Dam',\n",
    "    'Head Purge Position Z Collect Result_Dam',\n",
    "    'Head Zero Position Y Collect Result_Dam',\n",
    "    'Head Zero Position Z Collect Result_Dam',\n",
    "    'Machine Tact time Collect Result_Dam', 'PalletID',\n",
    "    'Production Qty Collect Result_Dam', 'Receip No',\n",
    "    'Stage1 Circle1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle4 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle4 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle4 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
    "    'THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam',\n",
    "    'THICKNESS 3 Collect Result_Dam',\n",
    "    'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    "    'Head Purge Position Z Collect Result_Fill1',\n",
    "    'CURE SPEED Collect Result_Fill2', 'inconsistant', 'cure_x_dist_dam',\n",
    "    'cure_z_dist_dam', 'cure_z_dist_fill2', 'Minus1_Dam', 'Minus2_Dam',\n",
    "    'Minus1_Fill1', 'Minus2_Fill1',\n",
    "    'Minus1Y_Dam', 'Minus2Y_Dam',\n",
    "    'Minus1Y_Fill1', 'Minus2Y_Fill1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2932d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans에 사용할 칼럼만 불러오기\n",
    "use_train = df_train[cluster_col]\n",
    "use_test = df_test[cluster_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7d414e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# best_k = 2 # centroid의 best 개수 -> elbow, 실루엣 두 방법을 비교하여 선정한다.\n",
    "# abnormal = 1 # 시뮬레이션에서 이상치 판독 기준 개수\n",
    "\n",
    "# # Best k 적합하기\n",
    "# best = KMeans(\n",
    "#         n_clusters = best_k,    # 클러스터 수 설정\n",
    "#         init=\"random\",   # 초기 중심점을 무작위로 선택\n",
    "#         n_init=\"auto\",    # k-means 알고리즘 반복 실행 횟수 설정\n",
    "#         random_state=42\n",
    "#     ).fit(use_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee75d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['kmeans_2'] = best.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7244731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['kmeans_2'] = best.predict(use_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "462efc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "best_k = 5 # centroid의 best 개수 -> elbow, 실루엣 두 방법을 비교하여 선정한다.\n",
    "abnormal = 1 # 시뮬레이션에서 이상치 판독 기준 개수\n",
    "\n",
    "# Best k 적합하기\n",
    "best = KMeans(\n",
    "        n_clusters = best_k,    # 클러스터 수 설정\n",
    "        init=\"random\",   # 초기 중심점을 무작위로 선택\n",
    "        n_init=\"auto\",    # k-means 알고리즘 반복 실행 횟수 설정\n",
    "        random_state=42\n",
    "    ).fit(use_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3ef26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['kmeans_5'] = best.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cfe3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['kmeans_5'] = best.predict(use_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dc64a",
   "metadata": {},
   "source": [
    "#### autoclave kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b44b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_autoclave = [\n",
    "#     '1st Pressure Collect Result_AutoClave',\n",
    "#     '2nd Pressure Collect Result_AutoClave',\n",
    "#     '3rd Pressure Collect Result_AutoClave',\n",
    "#     'Chamber Temp. Collect Result_AutoClave',\n",
    "#     'rount_1st_time', 'rount_2nd_time', 'rount_3rd_time', 'all_time',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5026bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kmeans에 사용할 칼럼만 불러오기\n",
    "# use_train = df_train[used_autoclave]\n",
    "# use_test = df_test[used_autoclave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffe43612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# best_k = 10 # centroid의 best 개수 -> elbow, 실루엣 두 방법을 비교하여 선정한다.\n",
    "# abnormal = 1 # 시뮬레이션에서 이상치 판독 기준 개수\n",
    "\n",
    "# # Best k 적합하기\n",
    "# best = KMeans(\n",
    "#         n_clusters = best_k,    # 클러스터 수 설정\n",
    "#         init=\"random\",   # 초기 중심점을 무작위로 선택\n",
    "#         n_init=\"auto\",    # k-means 알고리즘 반복 실행 횟수 설정\n",
    "#         random_state=42\n",
    "#     ).fit(use_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d3de19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['kmeans_2_autoclave'] = best.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf3a0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['kmeans_2_autoclave'] = best.predict(use_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ecde90",
   "metadata": {},
   "source": [
    "### Columns Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7952ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수많은 칼럼 버리기\n",
    "df_train = df_train.drop(columns = drop_col, axis = 1)\n",
    "df_test = df_test.drop(columns = drop_col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304c102",
   "metadata": {},
   "source": [
    "### Type 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfbf219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "categorical_features = ['Workorder_Dam', 'Model.Suffix_Dam']\n",
    "\n",
    "# 시드 설정\n",
    "np.random.seed(42)\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_train[feature] = le.fit_transform(df_train[feature])\n",
    "    \n",
    "    # 검증 데이터에 있는 새로운 값에 대해 처리\n",
    "    unique_values = set(df_test[feature].unique()) - set(le.classes_)\n",
    "    if unique_values:\n",
    "        # 새로운 값들을 인코딩할 무작위 숫자 생성\n",
    "        new_labels = np.random.randint(0, len(le.classes_), size=len(unique_values))\n",
    "        # 새로운 값들을 인코딩\n",
    "        le.classes_ = np.append(le.classes_, list(unique_values))\n",
    "        le.transform(list(unique_values))  # transform을 호출해서 classes_ 업데이트\n",
    "    \n",
    "    df_test[feature] = le.transform(df_test[feature])\n",
    "    label_encoders[feature] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2543ee",
   "metadata": {},
   "source": [
    "### target 0, 1 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07b35369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = np.where(df_train['target'] == 'Normal', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44166a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model.Suffix_Dam', 'Workorder_Dam', 'CURE SPEED Collect Result_Dam',\n",
       "       'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
       "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
       "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
       "       'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
       "       'Dispense Volume(Stage1) Collect Result_Dam',\n",
       "       'Dispense Volume(Stage2) Collect Result_Dam',\n",
       "       'Dispense Volume(Stage3) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
       "       'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam',\n",
       "       'Head Clean Position Z Collect Result_Dam',\n",
       "       'Head Purge Position Z Collect Result_Dam',\n",
       "       'Head Zero Position Y Collect Result_Dam',\n",
       "       'Head Zero Position Z Collect Result_Dam',\n",
       "       'Machine Tact time Collect Result_Dam',\n",
       "       'Production Qty Collect Result_Dam',\n",
       "       'Stage1 Circle1 Distance Speed Collect Result_Dam',\n",
       "       'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
       "       'Stage3 Circle1 Distance Speed Collect Result_Dam',\n",
       "       'THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam',\n",
       "       'THICKNESS 3 Collect Result_Dam',\n",
       "       'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
       "       'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
       "       'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
       "       'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
       "       'Dispense Volume(Stage1) Collect Result_Fill1',\n",
       "       'Dispense Volume(Stage2) Collect Result_Fill1',\n",
       "       'Dispense Volume(Stage3) Collect Result_Fill1',\n",
       "       'Head Purge Position Z Collect Result_Fill1',\n",
       "       'Machine Tact time Collect Result_Fill1',\n",
       "       'CURE END POSITION Z Collect Result_Fill2',\n",
       "       'CURE SPEED Collect Result_Fill2',\n",
       "       'CURE START POSITION Z Collect Result_Fill2',\n",
       "       'Machine Tact time Collect Result_Fill2', 'target', 'inconsistant',\n",
       "       'Stage1 Line diffent Distance Speed_Dam', 'Stage1 Line Sum Speed_Dam',\n",
       "       'Stage2 Line diffent Distance Speed_Dam', 'Stage2 Line Sum Speed_Dam',\n",
       "       'Stage3 Line diffent Distance Speed_Dam', 'Stage3 Line Sum Speed_Dam',\n",
       "       'rount_1st_time', 'rount_2nd_time', 'rount_3rd_time', 'Receip No',\n",
       "       'Equipment', 'PalletID', '1st Pressure x Time x Temp AutoClave',\n",
       "       '2nd Pressure x Time x Temp AutoClave',\n",
       "       '3rd Pressure x Time x Temp AutoClave', 'Minus1_Dam', 'Minus2_Dam',\n",
       "       'Minus1_Fill1', 'Minus2_Fill1', 'Minus1Y_Dam', 'Minus2Y_Dam',\n",
       "       'Minus1Y_Fill1', 'Minus2Y_Fill1', 'kmeans_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462174b9",
   "metadata": {},
   "source": [
    "## 5. 데이터 학습하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff8d21",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8477ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 리스트\n",
    "columns_to_convert = [\n",
    "    'Receip No', 'Equipment', 'PalletID', 'Model.Suffix_Dam', 'Workorder_Dam'\n",
    "]  \n",
    "\n",
    "# 변환할 컬럼명 리스트\n",
    "columns_to = [\n",
    "    'Head Zero Position Y Collect Result_Dam',\n",
    "    'Head Zero Position Z Collect Result_Dam',\n",
    "    'Head Clean Position Z Collect Result_Dam',\n",
    "    'Head Purge Position Z Collect Result_Dam',\n",
    "#     'Head Purge Position Z Collect Result_Fill1',\n",
    "    'CURE START POSITION Z Collect Result_Fill2',\n",
    "    'CURE END POSITION Z Collect Result_Fill2',\n",
    "    'CURE SPEED Collect Result_Fill2',\n",
    "    'Stage1 Circle1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line diffent Distance Speed_Dam',\n",
    "    'Stage2 Line diffent Distance Speed_Dam',\n",
    "    'Stage3 Line diffent Distance Speed_Dam',\n",
    "    'Stage1 Line Sum Speed_Dam',\n",
    "    'Stage2 Line Sum Speed_Dam',\n",
    "    'Stage3 Line Sum Speed_Dam',\n",
    "#     'cure_x_dist_dam',\n",
    "#     'cure_z_dist_dam',\n",
    "#     'cure_z_dist_fill2',\n",
    "    'kmeans_5',\n",
    "#     'kmeans_2',\n",
    "#     'kmeans_2_autoclave',\n",
    "    'Minus1Y_Dam', 'Minus2Y_Dam',\n",
    "    'rount_1st_time', 'rount_2nd_time', 'rount_3rd_time',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da47181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_indices = [\n",
    "    'Receip No', 'Equipment', 'PalletID', 'Model.Suffix_Dam', 'Workorder_Dam'\n",
    "] + columns_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "439a1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "cat_train, cat_test = variable_setting('catboost', df_train, df_test, columns_to_convert, columns_to)\n",
    "lgbm_train, lgbm_test = variable_setting('lightgbm', df_train, df_test, columns_to_convert, columns_to)\n",
    "xgb_train, xgb_test = variable_setting('xgboost', df_train, df_test, columns_to_convert, columns_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246e20e",
   "metadata": {},
   "source": [
    "### Best_Params 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58efe419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-22 07:20:31,928] A new study created in memory with name: no-name-fcf16c03-efb1-40dd-a8dc-4751197cefb7\n",
      "[I 2024-08-22 07:20:58,315] Trial 0 finished with value: 0.11976047904191617 and parameters: {'iterations': 437, 'depth': 10, 'learning_rate': 0.15702970884055384, 'l2_leaf_reg': 5.990598257128395, 'border_count': 66, 'random_strength': 1.559945204206032, 'bagging_temperature': 0.05808361216819946, 'od_type': 'IncToDec', 'od_wait': 39, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.11976047904191617.\n",
      "[I 2024-08-22 07:21:27,211] Trial 1 finished with value: 0.04573804573804574 and parameters: {'iterations': 850, 'depth': 5, 'learning_rate': 0.0035113563139704067, 'l2_leaf_reg': 1.8422110534358038, 'border_count': 100, 'random_strength': 5.247564316797622, 'bagging_temperature': 0.43194501864211576, 'od_type': 'Iter', 'od_wait': 15, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.11976047904191617.\n",
      "[I 2024-08-22 07:22:54,153] Trial 2 finished with value: 0.10101010101010102 and parameters: {'iterations': 510, 'depth': 9, 'learning_rate': 0.00397211072738191, 'l2_leaf_reg': 5.1472020397519795, 'border_count': 164, 'random_strength': 0.4645041281535269, 'bagging_temperature': 0.6075448519014384, 'od_type': 'IncToDec', 'od_wait': 48, 'boosting_type': 'Ordered'}. Best is trial 0 with value: 0.11976047904191617.\n",
      "[I 2024-08-22 07:23:06,896] Trial 3 finished with value: 0.12 and parameters: {'iterations': 374, 'depth': 4, 'learning_rate': 0.11290133559092672, 'l2_leaf_reg': 4.407123412458617, 'border_count': 59, 'random_strength': 4.951769101617525, 'bagging_temperature': 0.034388521115218396, 'od_type': 'IncToDec', 'od_wait': 37, 'boosting_type': 'Plain'}. Best is trial 3 with value: 0.12.\n",
      "[I 2024-08-22 07:23:10,001] Trial 4 finished with value: 0.12301587301587301 and parameters: {'iterations': 592, 'depth': 5, 'learning_rate': 0.8105016126411579, 'l2_leaf_reg': 7.7535769053775345, 'border_count': 242, 'random_strength': 8.94827350438166, 'bagging_temperature': 0.5978999788110851, 'od_type': 'IncToDec', 'od_wait': 18, 'boosting_type': 'Plain'}. Best is trial 4 with value: 0.12301587301587301.\n",
      "[I 2024-08-22 07:23:20,900] Trial 5 finished with value: 0.12375249500998003 and parameters: {'iterations': 450, 'depth': 5, 'learning_rate': 0.3063462210622082, 'l2_leaf_reg': 3.573965733668957, 'border_count': 94, 'random_strength': 5.426960832039788, 'bagging_temperature': 0.14092422497476265, 'od_type': 'IncToDec', 'od_wait': 50, 'boosting_type': 'Ordered'}. Best is trial 5 with value: 0.12375249500998003.\n",
      "[I 2024-08-22 07:23:29,183] Trial 6 finished with value: 0.12 and parameters: {'iterations': 104, 'depth': 9, 'learning_rate': 0.13199942261535016, 'l2_leaf_reg': 7.292781608729463, 'border_count': 204, 'random_strength': 0.7404465182668589, 'bagging_temperature': 0.3584657285442726, 'od_type': 'Iter', 'od_wait': 35, 'boosting_type': 'Ordered'}. Best is trial 5 with value: 0.12375249500998003.\n",
      "[I 2024-08-22 07:23:53,256] Trial 7 finished with value: 0.12698412698412698 and parameters: {'iterations': 380, 'depth': 6, 'learning_rate': 0.15446089075047073, 'l2_leaf_reg': 6.379199138838579, 'border_count': 230, 'random_strength': 4.722149252147278, 'bagging_temperature': 0.1195942459383017, 'od_type': 'Iter', 'od_wait': 33, 'boosting_type': 'Ordered'}. Best is trial 7 with value: 0.12698412698412698.\n",
      "[I 2024-08-22 07:24:13,647] Trial 8 finished with value: 0.0 and parameters: {'iterations': 570, 'depth': 6, 'learning_rate': 0.0011919481947918731, 'l2_leaf_reg': 1.0878353556631115, 'border_count': 39, 'random_strength': 6.364104113001393, 'bagging_temperature': 0.3143559810763267, 'od_type': 'Iter', 'od_wait': 20, 'boosting_type': 'Plain'}. Best is trial 7 with value: 0.12698412698412698.\n",
      "[I 2024-08-22 07:24:28,579] Trial 9 finished with value: 0.03347280334728033 and parameters: {'iterations': 306, 'depth': 4, 'learning_rate': 0.0074003857590873735, 'l2_leaf_reg': 1.6206006596675042, 'border_count': 240, 'random_strength': 8.08120379583605, 'bagging_temperature': 0.6334037565104235, 'od_type': 'IncToDec', 'od_wait': 17, 'boosting_type': 'Ordered'}. Best is trial 7 with value: 0.12698412698412698.\n",
      "[I 2024-08-22 07:26:05,082] Trial 10 finished with value: 0.12749003984063745 and parameters: {'iterations': 800, 'depth': 7, 'learning_rate': 0.031051095169832947, 'l2_leaf_reg': 9.323580515698755, 'border_count': 175, 'random_strength': 3.005648231192895, 'bagging_temperature': 0.9597707459454197, 'od_type': 'Iter', 'od_wait': 27, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:27:50,382] Trial 11 finished with value: 0.12375249500998003 and parameters: {'iterations': 992, 'depth': 7, 'learning_rate': 0.025182780327334946, 'l2_leaf_reg': 9.821172567545853, 'border_count': 181, 'random_strength': 3.1828625019297947, 'bagging_temperature': 0.9909503209111175, 'od_type': 'Iter', 'od_wait': 27, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:29:09,853] Trial 12 finished with value: 0.12375249500998003 and parameters: {'iterations': 732, 'depth': 7, 'learning_rate': 0.03414254748487734, 'l2_leaf_reg': 9.416179460932371, 'border_count': 205, 'random_strength': 2.8962008578506273, 'bagging_temperature': 0.9130313467526872, 'od_type': 'Iter', 'od_wait': 28, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:29:39,568] Trial 13 finished with value: 0.10101010101010102 and parameters: {'iterations': 214, 'depth': 8, 'learning_rate': 0.03095209069239351, 'l2_leaf_reg': 7.921384155856422, 'border_count': 140, 'random_strength': 3.100372271745976, 'bagging_temperature': 0.7959371819182048, 'od_type': 'Iter', 'od_wait': 25, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:30:28,991] Trial 14 finished with value: 0.1272365805168986 and parameters: {'iterations': 703, 'depth': 6, 'learning_rate': 0.058926379980701264, 'l2_leaf_reg': 6.445595756730794, 'border_count': 137, 'random_strength': 6.956000682964389, 'bagging_temperature': 0.19321912554519183, 'od_type': 'Iter', 'od_wait': 10, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:32:00,210] Trial 15 finished with value: 0.10101010101010102 and parameters: {'iterations': 730, 'depth': 8, 'learning_rate': 0.014399019740410912, 'l2_leaf_reg': 8.768145291101577, 'border_count': 123, 'random_strength': 7.459450398393244, 'bagging_temperature': 0.23601689115810115, 'od_type': 'Iter', 'od_wait': 10, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:32:52,465] Trial 16 finished with value: 0.12698412698412698 and parameters: {'iterations': 710, 'depth': 6, 'learning_rate': 0.05175992268951684, 'l2_leaf_reg': 2.7669101688296514, 'border_count': 163, 'random_strength': 6.985077076273519, 'bagging_temperature': 0.8047828078277305, 'od_type': 'Iter', 'od_wait': 11, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:34:13,867] Trial 17 finished with value: 0.12375249500998003 and parameters: {'iterations': 900, 'depth': 8, 'learning_rate': 0.06430024554463516, 'l2_leaf_reg': 6.666401539263855, 'border_count': 116, 'random_strength': 9.226539927971608, 'bagging_temperature': 0.48539449398315065, 'od_type': 'Iter', 'od_wait': 43, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:35:39,952] Trial 18 finished with value: 0.10101010101010102 and parameters: {'iterations': 820, 'depth': 7, 'learning_rate': 0.013567415053503881, 'l2_leaf_reg': 8.44872209508006, 'border_count': 192, 'random_strength': 3.7949951823059243, 'bagging_temperature': 0.732568791718313, 'od_type': 'Iter', 'od_wait': 23, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:35:50,767] Trial 19 finished with value: 0.11623246492985972 and parameters: {'iterations': 640, 'depth': 6, 'learning_rate': 0.37834082471103786, 'l2_leaf_reg': 5.574177607784717, 'border_count': 147, 'random_strength': 1.9068600060590692, 'bagging_temperature': 0.25997423016857707, 'od_type': 'Iter', 'od_wait': 14, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-22 07:37:40,605] Trial 20 finished with value: 0.11623246492985972 and parameters: {'iterations': 958, 'depth': 7, 'learning_rate': 0.015933508690179468, 'l2_leaf_reg': 0.0809038437656433, 'border_count': 169, 'random_strength': 6.2103923702294015, 'bagging_temperature': 0.3816494115978136, 'od_type': 'Iter', 'od_wait': 22, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:38:20,776] Trial 21 finished with value: 0.12375249500998003 and parameters: {'iterations': 784, 'depth': 6, 'learning_rate': 0.0801495399578019, 'l2_leaf_reg': 6.445281449636791, 'border_count': 215, 'random_strength': 4.158607286575115, 'bagging_temperature': 0.16279707038708446, 'od_type': 'Iter', 'od_wait': 30, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.12749003984063745.\n",
      "[I 2024-08-22 07:38:35,221] Trial 22 finished with value: 0.13779527559055116 and parameters: {'iterations': 667, 'depth': 5, 'learning_rate': 0.2892320793535773, 'l2_leaf_reg': 4.24968520442755, 'border_count': 225, 'random_strength': 4.597962786395598, 'bagging_temperature': 0.12989251469031243, 'od_type': 'Iter', 'od_wait': 33, 'boosting_type': 'Ordered'}. Best is trial 22 with value: 0.13779527559055116.\n",
      "[I 2024-08-22 07:38:41,811] Trial 23 finished with value: 0.12350597609561752 and parameters: {'iterations': 635, 'depth': 5, 'learning_rate': 0.7870972186313576, 'l2_leaf_reg': 3.972038066838304, 'border_count': 252, 'random_strength': 1.9686734899196252, 'bagging_temperature': 0.0026820673230499026, 'od_type': 'Iter', 'od_wait': 42, 'boosting_type': 'Ordered'}. Best is trial 22 with value: 0.13779527559055116.\n",
      "[I 2024-08-22 07:38:54,482] Trial 24 finished with value: 0.12698412698412698 and parameters: {'iterations': 888, 'depth': 4, 'learning_rate': 0.31094223075114463, 'l2_leaf_reg': 3.270898863066413, 'border_count': 142, 'random_strength': 6.0032839781744265, 'bagging_temperature': 0.21073569690088284, 'od_type': 'Iter', 'od_wait': 33, 'boosting_type': 'Ordered'}. Best is trial 22 with value: 0.13779527559055116.\n",
      "[I 2024-08-22 07:39:34,922] Trial 25 finished with value: 0.12749003984063745 and parameters: {'iterations': 667, 'depth': 5, 'learning_rate': 0.05149903835525117, 'l2_leaf_reg': 4.631783434756671, 'border_count': 185, 'random_strength': 4.108562699992042, 'bagging_temperature': 0.5309284730043782, 'od_type': 'Iter', 'od_wait': 31, 'boosting_type': 'Ordered'}. Best is trial 22 with value: 0.13779527559055116.\n",
      "[I 2024-08-22 07:39:42,155] Trial 26 finished with value: 0.12326043737574552 and parameters: {'iterations': 782, 'depth': 5, 'learning_rate': 0.4744999699294938, 'l2_leaf_reg': 4.279945163183808, 'border_count': 221, 'random_strength': 3.7747580924736206, 'bagging_temperature': 0.5109854928364395, 'od_type': 'Iter', 'od_wait': 31, 'boosting_type': 'Ordered'}. Best is trial 22 with value: 0.13779527559055116.\n",
      "[I 2024-08-22 07:39:48,212] Trial 27 finished with value: 0.11623246492985972 and parameters: {'iterations': 662, 'depth': 4, 'learning_rate': 0.21339640763741172, 'l2_leaf_reg': 4.884455431574275, 'border_count': 191, 'random_strength': 2.5447060594003843, 'bagging_temperature': 0.7133372449394493, 'od_type': 'Iter', 'od_wait': 26, 'boosting_type': 'Plain'}. Best is trial 22 with value: 0.13779527559055116.\n",
      "[I 2024-08-22 07:40:21,939] Trial 28 finished with value: 0.10101010101010102 and parameters: {'iterations': 507, 'depth': 5, 'learning_rate': 0.0073180136180899125, 'l2_leaf_reg': 2.6807224141552517, 'border_count': 182, 'random_strength': 4.685978876317478, 'bagging_temperature': 0.5278190371905984, 'od_type': 'Iter', 'od_wait': 39, 'boosting_type': 'Ordered'}. Best is trial 22 with value: 0.13779527559055116.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# optuna tuning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cat_best_params, X_cat, y_cat, cat_train_index, cat_valid_index \u001b[38;5;241m=\u001b[39m \u001b[43mcatboost_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mcatboost_optuna\u001b[0;34m(train, cat_features_indices)\u001b[0m\n\u001b[1;32m     44\u001b[0m sampler \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     45\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 최적의 하이퍼파라미터 출력\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:225\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m         \u001b[43m_log_failed_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfrozen_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc_err\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexc_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_err_fail_exc_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue_or_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_or_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m STUDY_TELL_WARNING_KEY \u001b[38;5;129;01min\u001b[39;00m frozen_trial\u001b[38;5;241m.\u001b[39msystem_attrs:\n\u001b[1;32m    232\u001b[0m         _log_failed_trial(\n\u001b[1;32m    233\u001b[0m             frozen_trial,\n\u001b[1;32m    234\u001b[0m             frozen_trial\u001b[38;5;241m.\u001b[39msystem_attrs[STUDY_TELL_WARNING_KEY],\n\u001b[1;32m    235\u001b[0m             value_or_values\u001b[38;5;241m=\u001b[39mvalue_or_values,\n\u001b[1;32m    236\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py:257\u001b[0m, in \u001b[0;36m_log_failed_trial\u001b[0;34m(trial, message, exc_info, value_or_values)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_log_failed_trial\u001b[39m(\n\u001b[1;32m    252\u001b[0m     trial: FrozenTrial,\n\u001b[1;32m    253\u001b[0m     message: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;167;01mWarning\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m     exc_info: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    255\u001b[0m     value_or_values: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[43m_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrial \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m failed with parameters: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m because of the following error: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m failed with value \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trial\u001b[38;5;241m.\u001b[39mnumber, \u001b[38;5;28mrepr\u001b[39m(value_or_values)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/logging/__init__.py:1489\u001b[0m, in \u001b[0;36mLogger.warning\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;124;03mLog 'msg % args' with severity 'WARNING'.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03mlogger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(WARNING):\n\u001b[0;32m-> 1489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWARNING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/logging/__init__.py:1622\u001b[0m, in \u001b[0;36mLogger._log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_info, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1621\u001b[0m         exc_info \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n\u001b[0;32m-> 1622\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakeRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle(record)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/logging/__init__.py:1591\u001b[0m, in \u001b[0;36mLogger.makeRecord\u001b[0;34m(self, name, level, fn, lno, msg, args, exc_info, func, extra, sinfo)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmakeRecord\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, level, fn, lno, msg, args, exc_info,\n\u001b[1;32m   1586\u001b[0m                func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1587\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;124;03m    A factory method which can be overridden in subclasses to create\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;124;03m    specialized LogRecords.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1591\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[43m_logRecordFactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extra \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1594\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m extra:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/logging/__init__.py:313\u001b[0m, in \u001b[0;36mLogRecord.__init__\u001b[0;34m(self, name, level, pathname, lineno, msg, args, exc_info, func, sinfo, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevelname \u001b[38;5;241m=\u001b[39m \u001b[43mgetLevelName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevelno \u001b[38;5;241m=\u001b[39m level\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpathname \u001b[38;5;241m=\u001b[39m pathname\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/logging/__init__.py:138\u001b[0m, in \u001b[0;36mgetLevelName\u001b[0;34m(level)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mReturn the textual or numeric representation of logging level 'level'.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m'Level %s' % level is returned.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# See Issues #22386, #27937 and #29220 for why it's this way\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_levelToName\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optuna tuning\n",
    "cat_best_params, X_cat, y_cat, cat_train_index, cat_valid_index = catboost_optuna(cat_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f7098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "lgbm_best_params, X_lgbm, y_lgbm, lgbm_train_index, lgbm_valid_index = lightgbm_optuna(lgbm_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07a429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "xgb_best_params, X_xgb, y_xgb, xgb_train_index, xgb_valid_index = xgboost_optuna(xgb_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1d4a2",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c91eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "X_train_cat_last = cat_train.loc[cat_train_index, cat_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_cat_last = cat_train.loc[cat_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_cat_last = cat_train.loc[cat_valid_index, cat_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_cat_last = cat_train.loc[cat_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_cat = cat_train.loc[:, cat_train.columns.difference(['target'])]\n",
    "y_cat = cat_train.loc[:, 'target']\n",
    "\n",
    "X_test_cat = cat_test.loc[:, cat_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5306578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "X_train_lgbm_last = lgbm_train.loc[lgbm_train_index, lgbm_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_lgbm_last = lgbm_train.loc[lgbm_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_lgbm_last = lgbm_train.loc[lgbm_valid_index, lgbm_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_lgbm_last = lgbm_train.loc[lgbm_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_lgbm = lgbm_train.loc[:, lgbm_train.columns.difference(['target'])]\n",
    "y_lgbm = lgbm_train.loc[:, 'target']\n",
    "\n",
    "X_test_lgbm = lgbm_test.loc[:, lgbm_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "X_train_xgb_last = xgb_train.loc[xgb_train_index, xgb_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_xgb_last = xgb_train.loc[xgb_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_xgb_last = xgb_train.loc[xgb_valid_index, xgb_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_xgb_last = xgb_train.loc[xgb_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_xgb = xgb_train.loc[:, xgb_train.columns.difference(['target'])]\n",
    "y_xgb = xgb_train.loc[:, 'target']\n",
    "\n",
    "X_test_xgb = xgb_test.loc[:, xgb_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389b44b",
   "metadata": {},
   "source": [
    "#### Startified CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc246d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 지정\n",
    "# gap: train 이후 몇개를 사용하지 않을것인지 정하기 위한 파라미터\n",
    "tscv = StratifiedKFold(n_splits = 5, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c626700",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04568426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified cv fitting models\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_cat_train = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_cat_last, y_train_cat_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    cat_best_params[\"random_seed\"] = 42\n",
    "    cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "    \n",
    "    \n",
    "    # fit the model\n",
    "    cat_best_model.fit(\n",
    "        X_train_cat_last.iloc[train_idx], y_train_cat_last[train_idx],\n",
    "        eval_set = [(X_train_cat_last.iloc[valid_idx], y_train_cat_last[valid_idx])],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose = 100, cat_features=cat_features_indices\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_cat_train.append(cat_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(cat_best_model.feature_importances_, index=list(X_train_cat_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c883c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_cat_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_cat_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "cat_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_cat_last, cat_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_cat = (cat_proba >= cat_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51062af",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_cat_last, y_pred_custom_threshold_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e34c8f",
   "metadata": {},
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified cv fitting models\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_lgbm_train = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_lgbm_last, y_train_lgbm_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    lgbm_best_params[\"random_seed\"] = 42\n",
    "    lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    lgbm_best_model.fit(\n",
    "        X_train_lgbm_last.iloc[train_idx], y_train_lgbm_last[train_idx],\n",
    "        eval_set = [(X_train_lgbm_last.iloc[valid_idx], y_train_lgbm_last[valid_idx])],\n",
    "        categorical_feature=cat_features_indices,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_lgbm_train.append(lgbm_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(lgbm_best_model.feature_importances_, index=list(X_train_lgbm_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e64b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_lgbm_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_lgbm_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "lgbm_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa51d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_lgbm_last, lgbm_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "lgbm_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_lgbm = (lgbm_proba >= lgbm_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd95ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_lgbm_last, y_pred_custom_threshold_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f2298",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673b044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified cv fitting models_xgb_train\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_xgb_train = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_xgb_last, y_train_xgb_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    xgb_best_params[\"random_seed\"] = 42\n",
    "    xgb_best_model = XGBClassifier(**xgb_best_params, early_stopping_rounds = 50, enable_categorical=True,)\n",
    "\n",
    "    # fit the model\n",
    "    xgb_best_model.fit(\n",
    "        X_train_xgb_last.iloc[train_idx], y_train_xgb_last[train_idx],\n",
    "        eval_set = [(X_train_xgb_last.iloc[valid_idx], y_train_xgb_last[valid_idx])],\n",
    "        verbose = 100\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_xgb_train.append(xgb_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(xgb_best_model.feature_importances_, index=list(X_train_xgb_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9850c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_xgb_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_xgb_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "xgb_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef05dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_xgb_last, xgb_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "xgb_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_xgb = (xgb_proba >= xgb_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_xgb_last, y_pred_custom_threshold_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4023a84a",
   "metadata": {},
   "source": [
    "#### Ensemble tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_test(last_num, test_set, prob):\n",
    "\n",
    "    from itertools import product\n",
    "    col_n = len(prob.columns)\n",
    "    row_n = len(prob)\n",
    "    \n",
    "    \n",
    "    # best ensemble\n",
    "    best_f1 = 0\n",
    "    best_f1_t = 0\n",
    "    best_weights = None\n",
    "    best_weights_t = None\n",
    "    A = [i for i in range(last_num + 1)]\n",
    "    \n",
    "    for w in tqdm(product(A, repeat = col_n)):\n",
    "        \n",
    "        # 가중치 열 만들기\n",
    "        weight_frame = pd.DataFrame([w]*row_n, columns = prob.columns)\n",
    "        \n",
    "        # 가중치 곱해주기\n",
    "        prob_weight = prob * weight_frame\n",
    "        \n",
    "        # 가중치의 합\n",
    "        w_sum = sum(w)\n",
    "        \n",
    "        # 평균 계산해주기\n",
    "        final_proba = np.sum(prob_weight/w_sum, axis = 1)\n",
    "        \n",
    "        # 가중 평균 계산\n",
    "        y_pred = (final_proba > 0.5).astype(int)\n",
    "\n",
    "        # F1 스코어 계산\n",
    "        f1 = f1_score(test_set, y_pred)\n",
    "\n",
    "        # Threshold 스코어 계산\n",
    "        precision, recall, thresholds = precision_recall_curve(test_set, final_proba)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold = (final_proba >= best_threshold).astype(int)\n",
    "        f1_t = f1_score(test_set, y_pred_custom_threshold)\n",
    "\n",
    "        # 최고 성능 저장\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_weights = w\n",
    "\n",
    "        if f1_t > best_f1_t:\n",
    "            best_f1_t = f1_t\n",
    "            best_weights_t = w\n",
    "            \n",
    "#         print(f'weight: {w}, best_f1: {best_f1}, best_f1_t: {best_f1_t}, best_weights: {best_weights}, best_weights_t: {best_weights_t}')\n",
    "\n",
    "    print('종료되었습니다.')\n",
    "    return best_f1, best_f1_t, best_weights, best_weights_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b044dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용\n",
    "best_f1, best_f1_t, best_weights, best_weights_t = weight_test(last_num = 20, test_set = y_valid_xgb_last, prob = pd.DataFrame({'cat_proba': cat_proba, 'lgbm_proba': lgbm_proba, 'xgb_proba': xgb_proba}))\n",
    "print(f'best_f1: {best_f1}, best_f1_t: {best_f1_t}, best_weights: {best_weights}, best_weights_t: {best_weights_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best ensemble 적용\n",
    "cat = cat_proba * best_weights_t[0]\n",
    "lgbm = lgbm_proba * best_weights_t[1]\n",
    "xgb = xgb_proba * best_weights_t[2]\n",
    "y_best = (cat + lgbm + xgb)/(sum(best_weights_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf06d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_valid_xgb_last, y_best)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "weights_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold = (y_best >= weights_best_threshold).astype(int)\n",
    "get_clf_eval(y_valid_xgb_last, y_pred_custom_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a46bf",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4341ca",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96958d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_cat = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_cat, y_cat):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    cat_best_params[\"random_seed\"] = 42\n",
    "    cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    cat_best_model.fit(\n",
    "        X_cat.iloc[train_idx], y_cat[train_idx],\n",
    "        eval_set = [(X_cat.iloc[valid_idx], y_cat[valid_idx])],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose = 100, cat_features=cat_features_indices\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_cat.append(cat_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8aa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_cat):\n",
    "    pred_list.append(model.predict_proba(X_test_cat)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "cat_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(cat_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ecc91",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75486999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_lgbm = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_lgbm, y_lgbm):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    lgbm_best_params[\"random_seed\"] = 42\n",
    "    lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    lgbm_best_model.fit(\n",
    "        X_lgbm.iloc[train_idx], y_lgbm[train_idx],\n",
    "        eval_set = [(X_lgbm.iloc[valid_idx], y_lgbm[valid_idx])],\n",
    "        categorical_feature=cat_features_indices,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_lgbm.append(lgbm_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b956c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_lgbm):\n",
    "    pred_list.append(model.predict_proba(X_test_lgbm)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "lgbm_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(lgbm_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc94a1b",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_xgb = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_xgb, y_xgb):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    xgb_best_params[\"random_seed\"] = 42\n",
    "    xgb_best_model = XGBClassifier(**xgb_best_params, early_stopping_rounds = 50, enable_categorical=True)\n",
    "\n",
    "    # fit the model\n",
    "    xgb_best_model.fit(\n",
    "        X_xgb.iloc[train_idx], y_xgb[train_idx],\n",
    "        eval_set = [(X_xgb.iloc[valid_idx], y_xgb[valid_idx])],\n",
    "        verbose = 100\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_xgb.append(xgb_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201671fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_xgb):\n",
    "    pred_list.append(model.predict_proba(X_test_xgb)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "xgb_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(xgb_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2689da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "cat = cat_proba * best_weights_t[0]\n",
    "lgbm = lgbm_proba * best_weights_t[1]\n",
    "xgb = xgb_proba * best_weights_t[2]\n",
    "p = (cat + lgbm + xgb)/(sum(best_weights_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = np.where(p >= weights_best_threshold, 'AbNormal', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae366f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(p2, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76956451",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = np.where(cat_proba >= 0.5, 'AbNormal', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(p3, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cae330",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(p2, df_test['inconsistant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5b99d",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = p2\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c717cfe",
   "metadata": {},
   "source": [
    "# 꼭 df_sub['target'] 확인하고 제출하시오"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a48e6e",
   "metadata": {},
   "source": [
    "# 불량품 예측\n",
    "\n",
    "불량품을 예측하기 위해 다음과 같은 함수화 정리를 진행한다. 혼란을 막기 위해 모든 과정을 함수화 하기로 한다.  \n",
    "목차는 다음과 같다.\n",
    "\n",
    "- 1. Load packages & Data\n",
    "- 2. Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe380d",
   "metadata": {},
   "source": [
    "## Equip1 유의한 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5915364",
   "metadata": {},
   "source": [
    "'DISCHARGED SPEED OF RESIN Collect Result_Dam',  \n",
    "'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',  \n",
    "'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',  \n",
    "'Stage2 Circle1 Distance Speed Collect Result_Dam',  \n",
    "'THICKNESS 1 Collect Result_Dam',  \n",
    "'THICKNESS 2 Collect Result_Dam',  \n",
    "'THICKNESS 3 Collect Result_Dam',  \n",
    "'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',  \n",
    "'Dispense Volume(Stage3) Collect Result_Fill1',  \n",
    "'Stage2 Line diffent Distance Speed_Dam',  \n",
    "'round_1st_time',  \n",
    "'round_2nd_time'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0733caab",
   "metadata": {},
   "source": [
    "## Equip2 유의한 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546610dc",
   "metadata": {},
   "source": [
    "'Model.Suffix_Dam',  \n",
    "'DISCHARGED SPEED OF RESIN Collect Result_Dam',  \n",
    "'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',  \n",
    "'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',  \n",
    "'Head Zero Position Y Collect Result_Dam',  \n",
    "'Stage2 Circle1 Distance Speed Collect Result_Dam',  \n",
    "'DISCHARGED SPEED OF RESIN Collect Result_Fill1',  \n",
    "'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',  \n",
    "'Dispense Volume(Stage3) Collect Result_Fill1',  \n",
    "'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',  \n",
    "'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',  \n",
    "'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',  \n",
    "'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',  \n",
    "'Machine Tact time Collect Result_Fill1',  \n",
    "'CURE SPEED Collect Result_Fill2',  \n",
    "'CURE START POSITION Z Collect Result_Fill2',  \n",
    "'Stage2 Line diffent Distance Speed_Dam',  \n",
    "'round_1st_time',  \n",
    "'round_2nd_time',  \n",
    "'round_3rd_time',  \n",
    "'workorder_third'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab477ae",
   "metadata": {},
   "source": [
    "## 1. Load Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e5c4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### ide packages\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    recall_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier, plot_metric\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# tuning\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea357c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "load_dir = './data/'\n",
    "train = pd.read_csv(load_dir + \"train.csv\")\n",
    "test = pd.read_csv(load_dir + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d3439",
   "metadata": {},
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195e730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 스코어 지정하기\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1, average = 'binary')\n",
    "\n",
    "# 평가 매트릭 계산 결과 보여주기\n",
    "def get_clf_eval(y_test, y_pred=None):\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
    "\n",
    "    print(\"오차행렬:\\n\", confusion)\n",
    "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
    "    print(\"정밀도: {:.4f}\".format(precision))\n",
    "    print(\"재현율: {:.4f}\".format(recall))\n",
    "    print(\"F1: {:.4f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1867a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 공정 맞춤형 위치 옮기기\n",
    "def move_data(data):\n",
    "    # divide\n",
    "    dam = data.filter(regex='_Dam')\n",
    "    fill1 = data.filter(regex='_Fill1')\n",
    "    fill2 = data.filter(regex='_Fill2')\n",
    "    autoclave = data.filter(regex='_AutoClave')\n",
    "    target = data['target']\n",
    "\n",
    "    # dam\n",
    "    dam = dam.dropna(axis=1, how='all')\n",
    "    dam = dam.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam')\n",
    "    dam_mask = dam[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan])].iloc[:, 24:].shift(-1, axis = 1).values\n",
    "    dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[24:]] = dam_mask\n",
    "    dam = dam.drop(columns='WorkMode Collect Result_Dam')\n",
    "\n",
    "    # fill1\n",
    "    fill1 = fill1.dropna(axis=1, how='all')\n",
    "    fill1 = fill1.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1')\n",
    "    fill1_mask = fill1[fill1['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan])].iloc[:, 14:].shift(-1, axis = 1).values\n",
    "    fill1.loc[fill1['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan]), fill1.columns[14:]] = fill1_mask\n",
    "    fill1 = fill1.drop(columns='WorkMode Collect Result_Fill1')\n",
    "\n",
    "    # fill2\n",
    "    fill2 = fill2.dropna(axis=1, how='all')\n",
    "    fill2 = fill2.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2')\n",
    "    fill2_mask = fill2[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan])].iloc[:, 24:].shift(-1, axis = 1).values\n",
    "    fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[24:]] = fill2_mask\n",
    "    fill2 = fill2.drop(columns='WorkMode Collect Result_Fill2')\n",
    "\n",
    "    # CONCAT\n",
    "    data = pd.concat([dam, fill1, fill2, autoclave, target], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fad315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dam, Fill1, Fill2에서 지정된 값이 다를 경우 Abnormal \n",
    "def inconsistant(data, columnname, iwantthiscolumnsname, is_train = True):\n",
    "    # 장비 번호가 다르면 불일치\n",
    "    if is_train:\n",
    "        cri = [\n",
    "            df_train[columnname + '_Dam'] != df_train[columnname + '_Fill1'],\n",
    "            df_train[columnname + '_Dam'] != df_train[columnname + '_Fill2'],\n",
    "            df_train[columnname + '_Fill1'] != df_train[columnname + '_Fill2'],\n",
    "            data[iwantthiscolumnsname] == 1\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        cri = [\n",
    "            df_test[columnname + '_Dam'] != df_test[columnname + '_Fill1'],\n",
    "            df_test[columnname + '_Dam'] != df_test[columnname + '_Fill2'],\n",
    "            df_test[columnname + '_Fill1'] != df_test[columnname + '_Fill1'],\n",
    "            data[iwantthiscolumnsname] == 1\n",
    "        ]\n",
    "    con = [1, 1, 1, 1]\n",
    "\n",
    "    data[iwantthiscolumnsname] = np.select(cri, con, default = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804698ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 세팅\n",
    "def variable_setting(types, tr, te, cat_col):\n",
    "    train = tr.copy()\n",
    "    test = te.copy()\n",
    "    \n",
    "    if types == 'catboost':\n",
    "        dtype = 'string'  # 원하는 데이터 타입\n",
    "        for column in cat_col:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "\n",
    "        dtype = 'category'  # 원하는 데이터 타입\n",
    "        for column in cat_col:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "    elif types == 'lightgbm':\n",
    "        dtype = 'float'  # 원하는 데이터 타입\n",
    "        for column in cat_col:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "\n",
    "        dtype = 'category'  # 원하는 데이터 타입\n",
    "        for column in cat_col:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "    elif types == 'xgboost':\n",
    "        dtype = 'float'  # 원하는 데이터 타입\n",
    "        for column in cat_col:\n",
    "            train[column] = train[column].astype(dtype)\n",
    "            test[column] = test[column].astype(dtype)\n",
    "            \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc05ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_best_threshold(model, X_valid, y_valid):\n",
    "    \n",
    "    # Precision - Recall\n",
    "    y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_valid, y_pred_proba)\n",
    "    f1_scores = 2*recall*precision / (recall + precision)\n",
    "    cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    y_pred_custom_threshold = (y_pred_proba >= cat_best_threshold).astype(int)\n",
    "    \n",
    "    return thresholds, y_pred_custom_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bee5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_optuna(train, cat_features_indices):\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "            'seed': 42,\n",
    "        }\n",
    "\n",
    "        model = XGBClassifier(eval_metric='logloss', **params, early_stopping_rounds = 50)\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 평가\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        # thresholds\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid, preds)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold_cat = (preds >= cat_best_threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred_custom_threshold_cat)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    # Optuna 스터디 생성 및 최적화\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "    \n",
    "#     study_best_trial_params = {\n",
    "#         'n_estimators': 389,\n",
    "#         'max_depth': 7,\n",
    "#         'learning_rate': 0.5624862523377674,\n",
    "#         'subsample': 0.6901977494513396,\n",
    "#         'colsample_bytree': 0.6626522287203345,\n",
    "#         'gamma': 1.5788663422268037,\n",
    "#         'lambda': 0.006161637899604562,\n",
    "#         'alpha': 0.040928088401419954\n",
    "#     }\n",
    "    \n",
    "    return study.best_trial.params, X, y, X_train.index, X_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f131662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_optuna(train, cat_features_indices):\n",
    "\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    def objective(trial):\n",
    "        lgbm_params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 400, 1500),\n",
    "            \"max_depth\": trial.suggest_int('max_depth', 3, 63),\n",
    "            \"learning_rate\": trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True), \n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "            \"min_child_weight\": trial.suggest_float('min_child_weight', 0.5, 4),\n",
    "            \"min_child_samples\": trial.suggest_int('min_child_samples', 5, 100),\n",
    "            \"subsample\": trial.suggest_float('subsample', 0.4, 1),\n",
    "            \"subsample_freq\": trial.suggest_int('subsample_freq', 0, 5),\n",
    "            \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.2, 1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 64),\n",
    "        }\n",
    "\n",
    "        model = LGBMClassifier(**lgbm_params, device='cpu', random_state=42, verbose=-1)\n",
    "\n",
    "        # 범주형 피처 적용\n",
    "        model.fit(X_train, y_train, categorical_feature=cat_features_indices,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 평가\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        # thresholds\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid, preds)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold_cat = (preds >= cat_best_threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred_custom_threshold_cat)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    # Optuna 스터디 생성 및 최적화\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "#     study_best_trial_params = {\n",
    "#         'n_estimators': 1181,\n",
    "#         'max_depth': 42,\n",
    "#         'learning_rate': 0.009444269250537143,\n",
    "#         'reg_alpha': 0.0358194186029774,\n",
    "#         'reg_lambda': 0.001046166296905767,\n",
    "#         'min_child_weight': 3.3802455616568117,\n",
    "#         'min_child_samples': 38,\n",
    "#         'subsample': 0.4032508293056122,\n",
    "#         'subsample_freq': 0,\n",
    "#         'colsample_bytree': 0.2541295048133576,\n",
    "#         'num_leaves': 62\n",
    "#     }\n",
    "        \n",
    "    return study.best_trial.params, X, y, X_train.index, X_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c3b707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_optuna(train, cat_features_indices):\n",
    "    \n",
    "    # train X, y\n",
    "    X = train.drop(columns=['target'])\n",
    "    y = train['target']\n",
    "\n",
    "    # $plit \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Pooling\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features_indices)\n",
    "    valid_pool = Pool(X_valid, y_valid, cat_features=cat_features_indices)\n",
    "    \n",
    "    # tuning parameters\n",
    "    def objective(trial):\n",
    "        # 하이퍼파라미터를 샘플링\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1.0, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0),\n",
    "            \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 1e-9, 10.0),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n",
    "            \"od_type\": trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "            \"od_wait\": trial.suggest_int(\"od_wait\", 10, 50),\n",
    "            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "#             \"scale_pos_weight\": trial.suggest_int('scale_pos_weight', 6, 10),\n",
    "            \"verbose\": 0,\n",
    "            \"random_seed\": 42,\n",
    "            'one_hot_max_size': 4\n",
    "        }\n",
    "\n",
    "        # CatBoost 모델 학습\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50, verbose=0)\n",
    "\n",
    "        # 검증 세트에 대한 예측 및 평가\n",
    "        preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "        # thresholds\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid, preds)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold_cat = (preds >= cat_best_threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_valid, y_pred_custom_threshold_cat)\n",
    "        \n",
    "        return f1\n",
    "\n",
    "    # Optuna 스터디 생성 및 최적화\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "#     study_best_trial_params = {\n",
    "#         'iterations': 800,\n",
    "#         'depth': 7,\n",
    "#         'learning_rate': 0.810235620776663,\n",
    "#         'l2_leaf_reg': 9.445546463334189,\n",
    "#         'border_count': 175,\n",
    "#         'random_strength': 9.076647952917511,\n",
    "#         'bagging_temperature': 0.940243954743633,\n",
    "#         'od_type': 'IncToDec',\n",
    "#         'od_wait': 23,\n",
    "#         'boosting_type': 'Plain'\n",
    "#     }\n",
    "    \n",
    "    return study.best_trial.params, X, y, X_train.index, X_valid.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443902e",
   "metadata": {},
   "source": [
    "## 3. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae0c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기준\n",
    "columnname = ['Equipment', 'Receip No Collect Result', 'Production Qty Collect Result', 'PalletID Collect Result', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4faaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop oolumns\n",
    "drop_col = [\n",
    "    \n",
    "    # 단일 칼럼\n",
    "    'Wip Line_Dam',\n",
    "    'Process Desc._Dam',\n",
    "    'Insp. Seq No._Dam',\n",
    "    'Insp Judge Code_Dam',\n",
    "    'Wip Line_Fill1',\n",
    "    'Process Desc._Fill1',\n",
    "    'Insp. Seq No._Fill1',\n",
    "    'Insp Judge Code_Fill1',\n",
    "    'Wip Line_Fill2',\n",
    "    'Process Desc._Fill2',\n",
    "    'Insp. Seq No._Fill2',\n",
    "    'Insp Judge Code_Fill2',\n",
    "    'Wip Line_AutoClave',\n",
    "    'Process Desc._AutoClave',\n",
    "    'Equipment_AutoClave',\n",
    "    'Insp. Seq No._AutoClave',\n",
    "    'Insp Judge Code_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Unit Time_AutoClave',\n",
    "    'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave',\n",
    "    \n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2',\n",
    "    'HEAD Standby Position X Collect Result_Fill2',\n",
    "    'HEAD Standby Position Y Collect Result_Fill2',\n",
    "    'HEAD Standby Position Z Collect Result_Fill2',\n",
    "    'Head Clean Position X Collect Result_Fill2',\n",
    "    'Head Clean Position Y Collect Result_Fill2',\n",
    "    'Head Clean Position Z Collect Result_Fill2',\n",
    "    'Head Purge Position X Collect Result_Fill2',\n",
    "    'Head Purge Position Y Collect Result_Fill2',\n",
    "    'Head Purge Position Z Collect Result_Fill2',\n",
    "    \n",
    "    'DISCHARGED SPEED OF RESIN Collect Result_Fill2',\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill2',\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill2',\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill2',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill2',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill2',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill2',\n",
    "    \n",
    "    'HEAD Standby Position X Collect Result_Dam',\n",
    "    'HEAD Standby Position Y Collect Result_Dam',\n",
    "    'HEAD Standby Position Z Collect Result_Dam',\n",
    "    'Head Clean Position X Collect Result_Dam',\n",
    "    'Head Clean Position Y Collect Result_Dam',\n",
    "    'Head Purge Position X Collect Result_Dam',\n",
    "    'Head Purge Position Y Collect Result_Dam',\n",
    "    'Head Zero Position X Collect Result_Dam',\n",
    "    'Head Zero Position Y Collect Result_Dam',\n",
    "    'Head Zero Position Z Collect Result_Dam',\n",
    "    \n",
    "    '1st Pressure Judge Value_AutoClave',\n",
    "    '2nd Pressure Judge Value_AutoClave',\n",
    "    '3rd Pressure Judge Value_AutoClave',\n",
    "    'Chamber Temp. Judge Value_AutoClave',\n",
    "    \n",
    "    'HEAD Standby Position X Collect Result_Fill1',\n",
    "    'HEAD Standby Position Y Collect Result_Fill1',\n",
    "    'HEAD Standby Position Z Collect Result_Fill1',\n",
    "    'Head Clean Position X Collect Result_Fill1',\n",
    "    'Head Clean Position Y Collect Result_Fill1',\n",
    "    'Head Clean Position Z Collect Result_Fill1',\n",
    "    'Head Purge Position X Collect Result_Fill1',\n",
    "    'Head Purge Position Y Collect Result_Fill1',\n",
    "\n",
    "    # Cure 변수는 거의 동일함 -> equipment별로 dam은 동일하기 때문에 자르기\n",
    "    'CURE END POSITION X Collect Result_Fill2',\n",
    "    'CURE END POSITION Z Collect Result_Fill2',\n",
    "    'CURE END POSITION Θ Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION X Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
    "    'CURE STANDBY POSITION Θ Collect Result_Fill2',\n",
    "    'CURE START POSITION X Collect Result_Fill2',\n",
    "    'CURE START POSITION Z Collect Result_Fill2',\n",
    "    'CURE START POSITION Θ Collect Result_Fill2',\n",
    "\n",
    "    'CURE END POSITION X Collect Result_Dam',\n",
    "    'CURE END POSITION Z Collect Result_Dam',\n",
    "    'CURE END POSITION Θ Collect Result_Dam',\n",
    "    'CURE STANDBY POSITION X Collect Result_Dam',\n",
    "    'CURE STANDBY POSITION Z Collect Result_Dam',\n",
    "    'CURE STANDBY POSITION Θ Collect Result_Dam',\n",
    "    'CURE START POSITION X Collect Result_Dam',\n",
    "    'CURE START POSITION Z Collect Result_Dam',\n",
    "    'CURE START POSITION Θ Collect Result_Dam',\n",
    "    \n",
    "    # 라인 서클 축약해서 넣어둠\n",
    "    'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Circle4 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Circle4 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Circle4 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "    'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
    "    \n",
    "    # 중복 변수\n",
    "    'PalletID Collect Result_Fill1',\n",
    "    'Production Qty Collect Result_Fill1',\n",
    "    'Receip No Collect Result_Fill1',\n",
    "    'PalletID Collect Result_Fill2',\n",
    "    'Production Qty Collect Result_Fill2',\n",
    "    'Receip No Collect Result_Fill2',\n",
    "    'Equipment_Fill1',\n",
    "    'Model.Suffix_Fill1',\n",
    "    'Workorder_Fill1',\n",
    "    'Equipment_Fill2',\n",
    "    'Model.Suffix_Fill2',\n",
    "    'Workorder_Fill2',\n",
    "    'Model.Suffix_AutoClave',\n",
    "    'Workorder_AutoClave',\n",
    "    'Workorder_Dam',\n",
    "    ####################################################################\n",
    "    # 새로운 변수(파생변수 생성 도중 제거하고 싶은 변수 넣기)\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2794d",
   "metadata": {},
   "source": [
    "## 4. Matched Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a483379",
   "metadata": {},
   "source": [
    "### 뒤로 밀린 데이터 원상복구 진행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12dda308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_318/691509256.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.     0.012  0.    ...  0.    -0.019  0.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[24:]] = dam_mask\n",
      "/tmp/ipykernel_318/691509256.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[114.612 114.612  85.    ...  85.    114.612  85.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[24:]] = fill2_mask\n",
      "/tmp/ipykernel_318/691509256.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.054  0.     0.    ...  0.     0.     0.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[24:]] = dam_mask\n",
      "/tmp/ipykernel_318/691509256.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[85. 85. 85. ... 85. 85. 85.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[24:]] = fill2_mask\n"
     ]
    }
   ],
   "source": [
    "# 위치 옮기기\n",
    "train_move = move_data(train)\n",
    "test_move = move_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12049aa2",
   "metadata": {},
   "source": [
    "### Modified Equipment data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0314fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment 번호만 가져오기\n",
    "train_move['Equipment_Dam'] = train_move['Equipment_Dam'].str.slice(15, 16)\n",
    "train_move['Equipment_Fill1'] = train_move['Equipment_Fill1'].str.slice(17, 18)\n",
    "train_move['Equipment_Fill2'] = train_move['Equipment_Fill2'].str.slice(17, 18)\n",
    "\n",
    "test_move['Equipment_Dam'] = test_move['Equipment_Dam'].str.slice(15, 16)\n",
    "test_move['Equipment_Fill1'] = test_move['Equipment_Fill1'].str.slice(17, 18)\n",
    "test_move['Equipment_Fill2'] = test_move['Equipment_Fill2'].str.slice(17, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c8dbe",
   "metadata": {},
   "source": [
    "### Type Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ac0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입 변경하기\n",
    "type_change = [\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "    'Equipment_Dam',\n",
    "    'Equipment_Fill1',\n",
    "    'Equipment_Fill2'\n",
    "]\n",
    "\n",
    "types = [\n",
    "    'float64', 'float64', 'float64', 'int64', 'int64', 'int64'\n",
    "]\n",
    "for i, t in zip(type_change, types):\n",
    "    train_move[i] = train_move[i].astype(t)\n",
    "    test_move[i] = test_move[i].astype(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba82c00",
   "metadata": {},
   "source": [
    "### Fill1의 X좌표 바꾸기\n",
    "\n",
    "- 바꿔야할 칼럼  \n",
    "DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1  \n",
    "DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1  \n",
    "DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1  \n",
    "Dispense Volume(Stage1) Collect Result_Fill1  \n",
    "Dispense Volume(Stage2) Collect Result_Fill1  \n",
    "Dispense Volume(Stage3) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1  \n",
    "HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24790210",
   "metadata": {},
   "source": [
    "### Equipment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c20f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100대 값을 갖는 X위치 1, 3 체인지\n",
    "condition = (train_move['Equipment_Fill1'] == 1) & (train_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] < 200)\n",
    "condition2 = (test_move['Equipment_Fill1'] == 1) & (test_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] < 200)\n",
    "\n",
    "# 바꿔야 되는 칼럼\n",
    "As = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "Bs = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "# 교환\n",
    "for a, b in zip(As, Bs):\n",
    "    train_move.loc[condition, [a, b]] = train_move.loc[condition, [b, a]].values\n",
    "    test_move.loc[condition2, [a, b]] = test_move.loc[condition2, [b, a]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d8197e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400대 값을 갖는 X위치 1, 2 체인지\n",
    "condition = (train_move['Equipment_Fill1'] == 1) & (train_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].between(400, 500))\n",
    "condition2 = (test_move['Equipment_Fill1'] == 1) & (test_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].between(400, 500))\n",
    "\n",
    "# 바꿔야 되는 칼럼\n",
    "As = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "Bs = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "# 교환\n",
    "for a, b in zip(As, Bs):\n",
    "    train_move.loc[condition, [a, b]] = train_move.loc[condition, [b, a]].values\n",
    "    test_move.loc[condition2, [a, b]] = test_move.loc[condition2, [b, a]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162a696",
   "metadata": {},
   "source": [
    "### Equipment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69733236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400대 값을 갖는 X위치 1, 2 체인지\n",
    "condition = (train_move['Equipment_Fill1'] == 2) & (train_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].between(400, 500))\n",
    "condition2 = (test_move['Equipment_Fill1'] == 2) & (test_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].between(400, 500))\n",
    "\n",
    "# 바꿔야 되는 칼럼\n",
    "As = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "Bs = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "# 교환\n",
    "for a, b in zip(As, Bs):\n",
    "    train_move.loc[condition, [a, b]] = train_move.loc[condition, [b, a]].values\n",
    "    test_move.loc[condition2, [a, b]] = test_move.loc[condition2, [b, a]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53b83a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100대 값을 갖는 X위치 1, 3 체인지\n",
    "condition = (train_move['Equipment_Fill1'] == 2) & (train_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] > 800)\n",
    "condition2 = (test_move['Equipment_Fill1'] == 2) & (test_move['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] > 800)\n",
    "\n",
    "# 바꿔야 되는 칼럼\n",
    "As = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "Bs = [\n",
    "    'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    "    'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    "    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'\n",
    "]\n",
    "\n",
    "# 교환\n",
    "for a, b in zip(As, Bs):\n",
    "    train_move.loc[condition, [a, b]] = train_move.loc[condition, [b, a]].values\n",
    "    test_move.loc[condition2, [a, b]] = test_move.loc[condition2, [b, a]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c94abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바뀐 데이터 이름 바꾸기\n",
    "df_train = train_move.copy()\n",
    "df_test = test_move.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc05aa",
   "metadata": {},
   "source": [
    "### New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "712525b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불일치 변수\n",
    "df_train['inconsistant'] = 0\n",
    "df_test['inconsistant'] = 0\n",
    "\n",
    "# 장착\n",
    "for i in columnname:\n",
    "    inconsistant(df_train, i, 'inconsistant', True)\n",
    "    inconsistant(df_test, i, 'inconsistant', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ed4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 0이하, 900이상인 값은 이상치로 분류\n",
    "for j in ['Machine Tact time Collect Result_Dam', 'Machine Tact time Collect Result_Fill1', 'Machine Tact time Collect Result_Fill2']:\n",
    "    cri = [\n",
    "        df_train[j] <= 0,\n",
    "        df_train[j] > 900\n",
    "    ]\n",
    "    cri2 = [\n",
    "        df_test[j] <= 0,\n",
    "        df_test[j] > 900\n",
    "    ]\n",
    "    con = [\n",
    "        1, 1\n",
    "    ]\n",
    "    df_train['inconsistant'] = np.select(cri, con, default = df_train['inconsistant'])\n",
    "    df_test['inconsistant'] = np.select(cri2, con, default = df_test['inconsistant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad0716",
   "metadata": {},
   "source": [
    "### Speed Line & Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b63870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라인별로 속도가 같아야 정상이다.\n",
    "df_train['Stage1 Line diffent Distance Speed_Dam'] = ((df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_train['Stage1 Line Sum Speed_Dam'] = df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_train['Stage2 Line diffent Distance Speed_Dam'] = ((df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_train['Stage2 Line Sum Speed_Dam'] = df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_train['Stage3 Line diffent Distance Speed_Dam'] = ((df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_train['Stage3 Line Sum Speed_Dam'] = df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line4 Distance Speed Collect Result_Dam']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라인별로 속도가 같아야 정상이다.\n",
    "df_test['Stage1 Line diffent Distance Speed_Dam'] = ((df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_test['Stage1 Line Sum Speed_Dam'] = df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['Stage2 Line diffent Distance Speed_Dam'] = ((df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_test['Stage2 Line Sum Speed_Dam'] = df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['Stage3 Line diffent Distance Speed_Dam'] = ((df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "df_test['Stage3 Line Sum Speed_Dam'] = df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line4 Distance Speed Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a9884",
   "metadata": {},
   "source": [
    "### time 보정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a2bfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time 보정하기\n",
    "df_train['round_1st_time'] = round(df_train['1st Pressure 1st Pressure Unit Time_AutoClave'], -1)\n",
    "df_train['round_2nd_time'] = round(df_train['2nd Pressure Unit Time_AutoClave'], -1)\n",
    "df_train['round_3rd_time'] = round(df_train['3rd Pressure Unit Time_AutoClave'], -1)\n",
    "df_train['all_time'] = round(df_train['Chamber Temp. Unit Time_AutoClave'], -1)\n",
    "\n",
    "df_test['round_1st_time'] = round(df_test['1st Pressure 1st Pressure Unit Time_AutoClave'], -1)\n",
    "df_test['round_2nd_time'] = round(df_test['2nd Pressure Unit Time_AutoClave'], -1)\n",
    "df_test['round_3rd_time'] = round(df_test['3rd Pressure Unit Time_AutoClave'], -1)\n",
    "df_test['all_time'] = round(df_test['Chamber Temp. Unit Time_AutoClave'], -1)\n",
    "\n",
    "time_col = [\n",
    "    '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "    '2nd Pressure Unit Time_AutoClave',\n",
    "    '3rd Pressure Unit Time_AutoClave',\n",
    "    'Chamber Temp. Unit Time_AutoClave'\n",
    "]\n",
    "\n",
    "# 적용\n",
    "df_train = df_train.drop(columns = time_col, axis = 1)\n",
    "df_test = df_test.drop(columns = time_col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2822a9",
   "metadata": {},
   "source": [
    "### Fill2 경화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c5a0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cure 위치 차이 즉 방향을 나타내는 변수 생성\n",
    "df_train['cure_x_direction_fill'] = np.where(df_train['CURE START POSITION X Collect Result_Fill2'] - df_train['CURE END POSITION X Collect Result_Fill2'] > 0, 1, -1)\n",
    "df_train['cure_y_dist_fill'] = df_train['CURE START POSITION Z Collect Result_Fill2'] - df_train['CURE END POSITION Z Collect Result_Fill2']\n",
    "\n",
    "df_test['cure_x_direction_fill'] = np.where(df_test['CURE START POSITION X Collect Result_Fill2'] - df_test['CURE END POSITION X Collect Result_Fill2'] > 0, 1, -1)\n",
    "df_test['cure_y_dist_fill'] = df_test['CURE START POSITION Z Collect Result_Fill2'] - df_test['CURE END POSITION Z Collect Result_Fill2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad827a",
   "metadata": {},
   "source": [
    "### 각 좌표별 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fe3f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Minus1_Dam']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Minus2_Dam']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Minus1_Dam']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Minus2_Dam']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Minus1_Fill1']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1']\n",
    "df_train['Minus2_Fill1']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_test['Minus1_Fill1']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1']\n",
    "df_test['Minus2_Fill1']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['Minus1Y_Dam']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Minus2Y_Dam']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Minus1Y_Dam']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Minus2Y_Dam']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Minus1Y_Fill1']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "df_train['Minus2Y_Fill1']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_test['Minus1Y_Fill1']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "df_test['Minus2Y_Fill1']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "df_train['Minus1Y_Dam'] = df_train['Minus1Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "df_train['Minus2Y_Dam'] = df_train['Minus2Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "\n",
    "df_test['Minus1Y_Dam'] = df_test['Minus1Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "df_test['Minus2Y_Dam'] = df_test['Minus2Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaeb695",
   "metadata": {},
   "source": [
    "### 타입 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c767600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af017c",
   "metadata": {},
   "source": [
    "### Workorder 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beb21a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['workorder_first'] = df_train['Workorder_Dam'].str.slice(0, 2)\n",
    "df_train['workorder_third'] = df_train['Workorder_Dam'].str.slice(2, 4)\n",
    "\n",
    "df_test['workorder_first'] = df_test['Workorder_Dam'].str.slice(0, 2)\n",
    "df_test['workorder_third'] = df_test['Workorder_Dam'].str.slice(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ecde90",
   "metadata": {},
   "source": [
    "### Columns Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7952ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수많은 칼럼 버리기\n",
    "df_train = df_train.drop(columns = drop_col, axis = 1)\n",
    "df_test = df_test.drop(columns = drop_col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304c102",
   "metadata": {},
   "source": [
    "### Type 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfbf219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "categorical_features = ['workorder_first', 'workorder_third', 'Model.Suffix_Dam']\n",
    "\n",
    "# 시드 설정\n",
    "np.random.seed(42)\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_train[feature] = le.fit_transform(df_train[feature])\n",
    "    \n",
    "    # 검증 데이터에 있는 새로운 값에 대해 처리\n",
    "    unique_values = set(df_test[feature].unique()) - set(le.classes_)\n",
    "    if unique_values:\n",
    "        # 새로운 값들을 인코딩할 무작위 숫자 생성\n",
    "        new_labels = np.random.randint(0, len(le.classes_), size=len(unique_values))\n",
    "        # 새로운 값들을 인코딩\n",
    "        le.classes_ = np.append(le.classes_, list(unique_values))\n",
    "        le.transform(list(unique_values))  # transform을 호출해서 classes_ 업데이트\n",
    "    \n",
    "    df_test[feature] = le.transform(df_test[feature])\n",
    "    label_encoders[feature] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2543ee",
   "metadata": {},
   "source": [
    "### target 0, 1 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07b35369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = np.where(df_train['target'] == 'Normal', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951eea9",
   "metadata": {},
   "source": [
    "### 이름 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07a344fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dic = {\n",
    "    'Equipment_Dam': 'equipment',\n",
    "    'Model.Suffix_Dam': 'model_suffix',\n",
    "    'Workorder_Dam': 'workorder',\n",
    "    'PalletID Collect Result_Dam':'pallet_id',\n",
    "    'Production Qty Collect Result_Dam': 'qty',\n",
    "    'Receip No Collect Result_Dam': 'receip'\n",
    "}\n",
    "\n",
    "df_train.rename(columns = name_dic, inplace = True)\n",
    "df_test.rename(columns = name_dic, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04f43922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect result 빼자\n",
    "df_train.columns = df_train.columns.str.replace(' Collect Result', '')\n",
    "df_test.columns = df_test.columns.str.replace(' Collect Result', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c2f19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM이 공백 넣지 말래요\n",
    "df_train.columns = df_train.columns.str.replace(' ', '_')\n",
    "df_test.columns = df_test.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb2b45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 애초에 비정상인 값은 굳이 학습시킬 이유 없다.\n",
    "df_train_adj = df_train[df_train['inconsistant'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f18698d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 집단으로 나누기\n",
    "equip1 = df_train_adj[df_train_adj['equipment'] == 1]\n",
    "equip2 = df_train_adj[df_train_adj['equipment'] == 2]\n",
    "\n",
    "equip1_test = df_test[df_test['equipment'] == 1]\n",
    "equip2_test = df_test[df_test['equipment'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ce605a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_318/4311638.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  equip1.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n",
      "/tmp/ipykernel_318/4311638.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  equip2.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n",
      "/tmp/ipykernel_318/4311638.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  equip1_test.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n",
      "/tmp/ipykernel_318/4311638.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  equip2_test.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# 집단을 나누는 기준을 제외시키기\n",
    "equip1.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n",
    "equip2.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n",
    "\n",
    "equip1_test.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)\n",
    "equip2_test.drop(['inconsistant', 'equipment'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462174b9",
   "metadata": {},
   "source": [
    "# Equipment 1\n",
    "\n",
    "## 5. 데이터 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ff826ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_318/1963253011.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  equip1.drop([\n"
     ]
    }
   ],
   "source": [
    "### 유의하지 않았던 변수 다 빼기\n",
    "equip1.drop([\n",
    "    'model_suffix',\n",
    "    'pallet_id',\n",
    "    'HEAD_NORMAL_COORDINATE_Y_AXIS(Stage2)_Fill1',\n",
    "    'CURE_SPEED_Fill2',\n",
    "    'cure_x_direction_fill',\n",
    "    'Minus1Y_Dam',\n",
    "    'Minus2Y_Dam'\n",
    "], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff8d21",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da47181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_indices = [\n",
    "      'receip', 'workorder_first', 'workorder_third', #'model_suffix', 'pallet_id', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "439a1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "cat_train, cat_test = variable_setting('catboost', equip1, equip1_test, cat_features_indices)\n",
    "lgbm_train, lgbm_test = variable_setting('lightgbm', equip1, equip1_test, cat_features_indices)\n",
    "xgb_train, xgb_test = variable_setting('xgboost', equip1, equip1_test, cat_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246e20e",
   "metadata": {},
   "source": [
    "### Best_Params 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efe419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-25 06:35:57,589] A new study created in memory with name: no-name-79bb3873-1daf-4442-a8bc-5fd253201f2e\n",
      "[I 2024-08-25 06:36:00,702] Trial 0 finished with value: 0.17606177606177606 and parameters: {'iterations': 437, 'depth': 10, 'learning_rate': 0.15702970884055384, 'l2_leaf_reg': 5.990598257128395, 'border_count': 66, 'random_strength': 1.559945204206032, 'bagging_temperature': 0.05808361216819946, 'od_type': 'IncToDec', 'od_wait': 39, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.17606177606177606.\n",
      "[I 2024-08-25 06:36:08,496] Trial 1 finished with value: 0.16823529411764707 and parameters: {'iterations': 850, 'depth': 5, 'learning_rate': 0.0035113563139704067, 'l2_leaf_reg': 1.8422110534358038, 'border_count': 100, 'random_strength': 5.247564316797622, 'bagging_temperature': 0.43194501864211576, 'od_type': 'Iter', 'od_wait': 15, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.17606177606177606.\n",
      "[I 2024-08-25 06:36:33,034] Trial 2 finished with value: 0.16791044776119404 and parameters: {'iterations': 510, 'depth': 9, 'learning_rate': 0.00397211072738191, 'l2_leaf_reg': 5.1472020397519795, 'border_count': 164, 'random_strength': 0.4645041281535269, 'bagging_temperature': 0.6075448519014384, 'od_type': 'IncToDec', 'od_wait': 48, 'boosting_type': 'Ordered'}. Best is trial 0 with value: 0.17606177606177606.\n",
      "[I 2024-08-25 06:36:34,610] Trial 3 finished with value: 0.17218543046357615 and parameters: {'iterations': 374, 'depth': 4, 'learning_rate': 0.11290133559092672, 'l2_leaf_reg': 4.407123412458617, 'border_count': 59, 'random_strength': 4.951769101617525, 'bagging_temperature': 0.034388521115218396, 'od_type': 'IncToDec', 'od_wait': 37, 'boosting_type': 'Plain'}. Best is trial 0 with value: 0.17606177606177606.\n",
      "[I 2024-08-25 06:36:35,265] Trial 4 finished with value: 0.17884405670665213 and parameters: {'iterations': 592, 'depth': 5, 'learning_rate': 0.8105016126411579, 'l2_leaf_reg': 7.7535769053775345, 'border_count': 242, 'random_strength': 8.94827350438166, 'bagging_temperature': 0.5978999788110851, 'od_type': 'IncToDec', 'od_wait': 18, 'boosting_type': 'Plain'}. Best is trial 4 with value: 0.17884405670665213.\n",
      "[I 2024-08-25 06:36:36,882] Trial 5 finished with value: 0.1793103448275862 and parameters: {'iterations': 450, 'depth': 5, 'learning_rate': 0.3063462210622082, 'l2_leaf_reg': 3.573965733668957, 'border_count': 94, 'random_strength': 5.426960832039788, 'bagging_temperature': 0.14092422497476265, 'od_type': 'IncToDec', 'od_wait': 50, 'boosting_type': 'Ordered'}. Best is trial 5 with value: 0.1793103448275862.\n",
      "[I 2024-08-25 06:36:41,947] Trial 6 finished with value: 0.17883755588673622 and parameters: {'iterations': 104, 'depth': 9, 'learning_rate': 0.13199942261535016, 'l2_leaf_reg': 7.292781608729463, 'border_count': 204, 'random_strength': 0.7404465182668589, 'bagging_temperature': 0.3584657285442726, 'od_type': 'Iter', 'od_wait': 35, 'boosting_type': 'Ordered'}. Best is trial 5 with value: 0.1793103448275862.\n",
      "[I 2024-08-25 06:36:44,219] Trial 7 finished with value: 0.1813633520950594 and parameters: {'iterations': 380, 'depth': 6, 'learning_rate': 0.15446089075047073, 'l2_leaf_reg': 6.379199138838579, 'border_count': 230, 'random_strength': 4.722149252147278, 'bagging_temperature': 0.1195942459383017, 'od_type': 'Iter', 'od_wait': 33, 'boosting_type': 'Ordered'}. Best is trial 7 with value: 0.1813633520950594.\n",
      "[I 2024-08-25 06:36:50,068] Trial 8 finished with value: 0.16584564860426929 and parameters: {'iterations': 570, 'depth': 6, 'learning_rate': 0.0011919481947918731, 'l2_leaf_reg': 1.0878353556631115, 'border_count': 39, 'random_strength': 6.364104113001393, 'bagging_temperature': 0.3143559810763267, 'od_type': 'Iter', 'od_wait': 20, 'boosting_type': 'Plain'}. Best is trial 7 with value: 0.1813633520950594.\n",
      "[I 2024-08-25 06:36:53,817] Trial 9 finished with value: 0.16611295681063123 and parameters: {'iterations': 306, 'depth': 4, 'learning_rate': 0.0074003857590873735, 'l2_leaf_reg': 1.6206006596675042, 'border_count': 240, 'random_strength': 8.08120379583605, 'bagging_temperature': 0.6334037565104235, 'od_type': 'IncToDec', 'od_wait': 17, 'boosting_type': 'Ordered'}. Best is trial 7 with value: 0.1813633520950594.\n",
      "[I 2024-08-25 06:37:06,706] Trial 10 finished with value: 0.18231540565177756 and parameters: {'iterations': 800, 'depth': 7, 'learning_rate': 0.031051095169832947, 'l2_leaf_reg': 9.323580515698755, 'border_count': 175, 'random_strength': 3.005648231192895, 'bagging_temperature': 0.9597707459454197, 'od_type': 'Iter', 'od_wait': 27, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.18231540565177756.\n",
      "[I 2024-08-25 06:37:21,129] Trial 11 finished with value: 0.17684887459807075 and parameters: {'iterations': 992, 'depth': 7, 'learning_rate': 0.025182780327334946, 'l2_leaf_reg': 9.821172567545853, 'border_count': 181, 'random_strength': 3.1828625019297947, 'bagging_temperature': 0.9909503209111175, 'od_type': 'Iter', 'od_wait': 27, 'boosting_type': 'Ordered'}. Best is trial 10 with value: 0.18231540565177756.\n",
      "[I 2024-08-25 06:37:34,003] Trial 12 finished with value: 0.1853211009174312 and parameters: {'iterations': 732, 'depth': 7, 'learning_rate': 0.03414254748487734, 'l2_leaf_reg': 9.416179460932371, 'border_count': 205, 'random_strength': 2.8962008578506273, 'bagging_temperature': 0.9130313467526872, 'od_type': 'Iter', 'od_wait': 28, 'boosting_type': 'Ordered'}. Best is trial 12 with value: 0.1853211009174312.\n",
      "[I 2024-08-25 06:37:53,154] Trial 13 finished with value: 0.18932874354561102 and parameters: {'iterations': 758, 'depth': 8, 'learning_rate': 0.02767134738259755, 'l2_leaf_reg': 9.964909287274866, 'border_count': 143, 'random_strength': 2.9441574133799593, 'bagging_temperature': 0.9913862056543774, 'od_type': 'Iter', 'od_wait': 28, 'boosting_type': 'Ordered'}. Best is trial 13 with value: 0.18932874354561102.\n"
     ]
    }
   ],
   "source": [
    "# optuna tuning\n",
    "cat_best_params, X_cat, y_cat, cat_train_index, cat_valid_index = catboost_optuna(cat_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f7098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "lgbm_best_params, X_lgbm, y_lgbm, lgbm_train_index, lgbm_valid_index = lightgbm_optuna(lgbm_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07a429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "xgb_best_params, X_xgb, y_xgb, xgb_train_index, xgb_valid_index = xgboost_optuna(xgb_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1d4a2",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c91eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "X_train_cat_last = cat_train.loc[cat_train_index, cat_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_cat_last = cat_train.loc[cat_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_cat_last = cat_train.loc[cat_valid_index, cat_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_cat_last = cat_train.loc[cat_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_cat = cat_train.loc[:, cat_train.columns.difference(['target'])]\n",
    "y_cat = cat_train.loc[:, 'target']\n",
    "\n",
    "X_test_cat = cat_test.loc[:, cat_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5306578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "X_train_lgbm_last = lgbm_train.loc[lgbm_train_index, lgbm_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_lgbm_last = lgbm_train.loc[lgbm_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_lgbm_last = lgbm_train.loc[lgbm_valid_index, lgbm_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_lgbm_last = lgbm_train.loc[lgbm_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_lgbm = lgbm_train.loc[:, lgbm_train.columns.difference(['target'])]\n",
    "y_lgbm = lgbm_train.loc[:, 'target']\n",
    "\n",
    "X_test_lgbm = lgbm_test.loc[:, lgbm_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "X_train_xgb_last = xgb_train.loc[xgb_train_index, xgb_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_xgb_last = xgb_train.loc[xgb_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_xgb_last = xgb_train.loc[xgb_valid_index, xgb_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_xgb_last = xgb_train.loc[xgb_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_xgb = xgb_train.loc[:, xgb_train.columns.difference(['target'])]\n",
    "y_xgb = xgb_train.loc[:, 'target']\n",
    "\n",
    "X_test_xgb = xgb_test.loc[:, xgb_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389b44b",
   "metadata": {},
   "source": [
    "#### Startified CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc246d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 지정\n",
    "# gap: train 이후 몇개를 사용하지 않을것인지 정하기 위한 파라미터\n",
    "tscv = StratifiedKFold(n_splits = 10, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c626700",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04568426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified cv fitting models\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_cat_train = []\n",
    "cat_best_params[\"random_seed\"] = 42\n",
    "cat_best_params['one_hot_max_size'] = 4\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_cat_last, y_train_cat_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "    \n",
    "    \n",
    "    # fit the model\n",
    "    cat_best_model.fit(\n",
    "        X_train_cat_last.iloc[train_idx], y_train_cat_last[train_idx],\n",
    "        eval_set = [(X_train_cat_last.iloc[valid_idx], y_train_cat_last[valid_idx])],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose = 100, cat_features=cat_features_indices\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_cat_train.append(cat_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(cat_best_model.feature_importances_, index=list(X_train_cat_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c883c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_cat_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_cat_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "cat_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_cat_last, cat_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_cat = (cat_proba >= cat_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51062af",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_cat_last, y_pred_custom_threshold_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e34c8f",
   "metadata": {},
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified cv fitting models\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_lgbm_train = []\n",
    "lgbm_best_params[\"random_seed\"] = 42\n",
    "lgbm_best_params['force_row_wise'] = True\n",
    "lgbm_best_params['is_unbalance'] = True\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_lgbm_last, y_train_lgbm_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    lgbm_best_model.fit(\n",
    "        X_train_lgbm_last.iloc[train_idx], y_train_lgbm_last[train_idx],\n",
    "        eval_set = [(X_train_lgbm_last.iloc[valid_idx], y_train_lgbm_last[valid_idx])],\n",
    "        categorical_feature=cat_features_indices,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_lgbm_train.append(lgbm_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(lgbm_best_model.feature_importances_, index=list(X_train_lgbm_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e64b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_lgbm_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_lgbm_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "lgbm_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa51d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_lgbm_last, lgbm_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "lgbm_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_lgbm = (lgbm_proba >= lgbm_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd95ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_lgbm_last, y_pred_custom_threshold_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f2298",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d673b044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified cv fitting models_xgb_train\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_xgb_train = []\n",
    "xgb_best_params[\"seed\"] = 42\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_xgb_last, y_train_xgb_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    xgb_best_model = XGBClassifier(**xgb_best_params, early_stopping_rounds = 50)\n",
    "\n",
    "    # fit the model\n",
    "    xgb_best_model.fit(\n",
    "        X_train_xgb_last.iloc[train_idx], y_train_xgb_last[train_idx],\n",
    "        eval_set = [(X_train_xgb_last.iloc[valid_idx], y_train_xgb_last[valid_idx])],\n",
    "        verbose = 100\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_xgb_train.append(xgb_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(xgb_best_model.feature_importances_, index=list(X_train_xgb_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9850c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_xgb_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_xgb_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "xgb_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef05dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_xgb_last, xgb_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "xgb_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_xgb = (xgb_proba >= xgb_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_xgb_last, y_pred_custom_threshold_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4023a84a",
   "metadata": {},
   "source": [
    "#### Ensemble tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_test(last_num, test_set, prob):\n",
    "\n",
    "    from itertools import product\n",
    "    col_n = len(prob.columns)\n",
    "    row_n = len(prob)\n",
    "    \n",
    "    \n",
    "    # best ensemble\n",
    "    best_f1 = 0\n",
    "    best_f1_t = 0\n",
    "    best_weights = None\n",
    "    best_weights_t = None\n",
    "    A = [i for i in range(last_num + 1)]\n",
    "    \n",
    "    for w in tqdm(product(A, repeat = col_n)):\n",
    "        \n",
    "        # 가중치 열 만들기\n",
    "        weight_frame = pd.DataFrame([w]*row_n, columns = prob.columns)\n",
    "        \n",
    "        # 가중치 곱해주기\n",
    "        prob_weight = prob * weight_frame\n",
    "        \n",
    "        # 가중치의 합\n",
    "        w_sum = sum(w)\n",
    "        \n",
    "        # 평균 계산해주기\n",
    "        final_proba = np.sum(prob_weight/w_sum, axis = 1)\n",
    "        \n",
    "        # 가중 평균 계산\n",
    "        y_pred = (final_proba > 0.5).astype(int)\n",
    "\n",
    "        # F1 스코어 계산\n",
    "        f1 = f1_score(test_set, y_pred)\n",
    "\n",
    "        # Threshold 스코어 계산\n",
    "        precision, recall, thresholds = precision_recall_curve(test_set, final_proba)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold = (final_proba >= best_threshold).astype(int)\n",
    "        f1_t = f1_score(test_set, y_pred_custom_threshold)\n",
    "\n",
    "        # 최고 성능 저장\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_weights = w\n",
    "\n",
    "        if f1_t > best_f1_t:\n",
    "            best_f1_t = f1_t\n",
    "            best_weights_t = w\n",
    "            \n",
    "#         print(f'weight: {w}, best_f1: {best_f1}, best_f1_t: {best_f1_t}, best_weights: {best_weights}, best_weights_t: {best_weights_t}')\n",
    "\n",
    "    print('종료되었습니다.')\n",
    "    return best_f1, best_f1_t, best_weights, best_weights_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b044dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용\n",
    "best_f1, best_f1_t, best_weights, best_weights_t = weight_test(last_num = 20, test_set = y_valid_xgb_last, prob = pd.DataFrame({'cat_proba': cat_proba, 'lgbm_proba': lgbm_proba, 'xgb_proba': xgb_proba}))\n",
    "print(f'best_f1: {best_f1}, best_f1_t: {best_f1_t}, best_weights: {best_weights}, best_weights_t: {best_weights_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380983f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best ensemble 적용\n",
    "cat = cat_proba * best_weights_t[0]\n",
    "lgbm = lgbm_proba * best_weights_t[1]\n",
    "xgb = xgb_proba * best_weights_t[2]\n",
    "y_best = (cat + lgbm + xgb)/(sum(best_weights_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf06d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_valid_xgb_last, y_best)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "weights_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold = (y_best >= weights_best_threshold).astype(int)\n",
    "get_clf_eval(y_valid_xgb_last, y_pred_custom_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a46bf",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4341ca",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96958d58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_cat = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_cat, y_cat):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    cat_best_params[\"random_seed\"] = 42\n",
    "    cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    cat_best_model.fit(\n",
    "        X_cat.iloc[train_idx], y_cat.iloc[train_idx],\n",
    "        eval_set = [(X_cat.iloc[valid_idx], y_cat.iloc[valid_idx])],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose = 100, cat_features=cat_features_indices\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_cat.append(cat_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8aa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_cat):\n",
    "    pred_list.append(model.predict_proba(X_test_cat[X_test_cat.columns.difference(['inconsistant'])])[:, 1])\n",
    "\n",
    "# 확률값 평균내기\n",
    "cat_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(cat_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ecc91",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75486999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_lgbm = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_lgbm, y_lgbm):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    lgbm_best_params[\"random_seed\"] = 42\n",
    "    lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    lgbm_best_model.fit(\n",
    "        X_lgbm.iloc[train_idx], y_lgbm.iloc[train_idx],\n",
    "        eval_set = [(X_lgbm.iloc[valid_idx], y_lgbm.iloc[valid_idx])],\n",
    "        categorical_feature=cat_features_indices,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_lgbm.append(lgbm_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b956c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_lgbm):\n",
    "    pred_list.append(model.predict_proba(X_test_lgbm[X_test_lgbm.columns.difference(['inconsistant'])])[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "lgbm_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(lgbm_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc94a1b",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74563a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_xgb = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_xgb, y_xgb):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    xgb_best_params[\"seed\"] = 42\n",
    "    xgb_best_model = XGBClassifier(**xgb_best_params, early_stopping_rounds = 50)\n",
    "\n",
    "    # fit the model\n",
    "    xgb_best_model.fit(\n",
    "        X_xgb.iloc[train_idx], y_xgb.iloc[train_idx],\n",
    "        eval_set = [(X_xgb.iloc[valid_idx], y_xgb.iloc[valid_idx])],\n",
    "        verbose = 100\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_xgb.append(xgb_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201671fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_xgb):\n",
    "    pred_list.append(model.predict_proba(X_test_xgb[X_test_xgb.columns.difference(['inconsistant'])])[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "xgb_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1584442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(xgb_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2689da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "cat = cat_proba * best_weights_t[0]\n",
    "lgbm = lgbm_proba * best_weights_t[1]\n",
    "xgb = xgb_proba * best_weights_t[2]\n",
    "p = (cat + lgbm + xgb)/(sum(best_weights_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_equip1 = np.where(p >= weights_best_threshold, 'AbNormal', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae366f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(p2_equip1, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76956451",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_equip1 = np.where(p >= 0.5, 'AbNormal', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(p3_equip1, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b0013",
   "metadata": {},
   "source": [
    "# Equipment 2\n",
    "\n",
    "## 5. 데이터 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb32f9b",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca680ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_indices = [\n",
    "      'model_suffix', 'pallet_id', 'receip', 'workorder_first', 'workorder_third'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "cat_train, cat_test = variable_setting('catboost', equip2, equip2_test, cat_features_indices)\n",
    "lgbm_train, lgbm_test = variable_setting('lightgbm', equip2, equip2_test, cat_features_indices)\n",
    "xgb_train, xgb_test = variable_setting('xgboost', equip2, equip2_test, cat_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02c39d",
   "metadata": {},
   "source": [
    "### Best_Params 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fd198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "cat_best_params, X_cat, y_cat, cat_train_index, cat_valid_index = catboost_optuna(cat_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818bad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "lgbm_best_params, X_lgbm, y_lgbm, lgbm_train_index, lgbm_valid_index = lightgbm_optuna(lgbm_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5427e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optuna tuning\n",
    "xgb_best_params, X_xgb, y_xgb, xgb_train_index, xgb_valid_index = xgboost_optuna(xgb_train, cat_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62108710",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038032aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "X_train_cat_last = cat_train.loc[cat_train_index, cat_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_cat_last = cat_train.loc[cat_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_cat_last = cat_train.loc[cat_valid_index, cat_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_cat_last = cat_train.loc[cat_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_cat = cat_train.loc[:, cat_train.columns.difference(['target'])]\n",
    "y_cat = cat_train.loc[:, 'target']\n",
    "\n",
    "X_test_cat = cat_test.loc[:, cat_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f007088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "X_train_lgbm_last = lgbm_train.loc[lgbm_train_index, lgbm_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_lgbm_last = lgbm_train.loc[lgbm_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_lgbm_last = lgbm_train.loc[lgbm_valid_index, lgbm_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_lgbm_last = lgbm_train.loc[lgbm_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_lgbm = lgbm_train.loc[:, lgbm_train.columns.difference(['target'])]\n",
    "y_lgbm = lgbm_train.loc[:, 'target']\n",
    "\n",
    "X_test_lgbm = lgbm_test.loc[:, lgbm_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112eed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "X_train_xgb_last = xgb_train.loc[xgb_train_index, xgb_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_train_xgb_last = xgb_train.loc[xgb_train_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_valid_xgb_last = xgb_train.loc[xgb_valid_index, xgb_train.columns.difference(['target'])].reset_index(drop=True)\n",
    "y_valid_xgb_last = xgb_train.loc[xgb_valid_index, 'target'].reset_index(drop=True)\n",
    "\n",
    "X_xgb = xgb_train.loc[:, xgb_train.columns.difference(['target'])]\n",
    "y_xgb = xgb_train.loc[:, 'target']\n",
    "\n",
    "X_test_xgb = xgb_test.loc[:, xgb_test.columns.difference(['Set ID', 'target'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd5632",
   "metadata": {},
   "source": [
    "#### Startified CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e251eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 지정\n",
    "# gap: train 이후 몇개를 사용하지 않을것인지 정하기 위한 파라미터\n",
    "tscv = StratifiedKFold(n_splits = 10, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff78a0",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8329a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified cv fitting models\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_cat_train = []\n",
    "cat_best_params[\"random_seed\"] = 42\n",
    "cat_best_params['one_hot_max_size'] = 4\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_cat_last, y_train_cat_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "    \n",
    "    \n",
    "    # fit the model\n",
    "    cat_best_model.fit(\n",
    "        X_train_cat_last.iloc[train_idx], y_train_cat_last[train_idx],\n",
    "        eval_set = [(X_train_cat_last.iloc[valid_idx], y_train_cat_last[valid_idx])],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose = 100, cat_features=cat_features_indices\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_cat_train.append(cat_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(cat_best_model.feature_importances_, index=list(X_train_cat_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff86e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_cat_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_cat_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "cat_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5709c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_cat_last, cat_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "cat_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_cat = (cat_proba >= cat_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb62da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_cat_last, y_pred_custom_threshold_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea8b62",
   "metadata": {},
   "source": [
    "#### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified cv fitting models\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_lgbm_train = []\n",
    "lgbm_best_params[\"random_seed\"] = 42\n",
    "lgbm_best_params['force_row_wise'] = True\n",
    "lgbm_best_params['is_unbalance'] = True\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_lgbm_last, y_train_lgbm_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    lgbm_best_model.fit(\n",
    "        X_train_lgbm_last.iloc[train_idx], y_train_lgbm_last[train_idx],\n",
    "        eval_set = [(X_train_lgbm_last.iloc[valid_idx], y_train_lgbm_last[valid_idx])],\n",
    "        categorical_feature=cat_features_indices,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_lgbm_train.append(lgbm_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(lgbm_best_model.feature_importances_, index=list(X_train_lgbm_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_lgbm_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_lgbm_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "lgbm_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b734f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_lgbm_last, lgbm_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "lgbm_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_lgbm = (lgbm_proba >= lgbm_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703dd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_lgbm_last, y_pred_custom_threshold_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03aec9",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f94bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stratified cv fitting models_xgb_train\n",
    "\n",
    "# 해당 모델 저장 리스트\n",
    "models_xgb_train = []\n",
    "xgb_best_params[\"seed\"] = 42\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_train_xgb_last, y_train_xgb_last):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    xgb_best_model = XGBClassifier(**xgb_best_params, early_stopping_rounds = 50)\n",
    "\n",
    "    # fit the model\n",
    "    xgb_best_model.fit(\n",
    "        X_train_xgb_last.iloc[train_idx], y_train_xgb_last[train_idx],\n",
    "        eval_set = [(X_train_xgb_last.iloc[valid_idx], y_train_xgb_last[valid_idx])],\n",
    "        verbose = 100\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_xgb_train.append(xgb_best_model)\n",
    "    \n",
    "    # 위 feature importance를 시각화해봅니다.\n",
    "    importances = pd.Series(xgb_best_model.feature_importances_, index=list(X_train_xgb_last.columns))\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    sns.barplot(x=importances, y=importances.index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_xgb_train):\n",
    "    pred_list.append(model.predict_proba(X_valid_xgb_last)[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "xgb_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_valid_xgb_last, xgb_proba)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "xgb_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold_xgb = (xgb_proba >= xgb_best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61901b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_valid_xgb_last, y_pred_custom_threshold_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e39d9",
   "metadata": {},
   "source": [
    "#### Ensemble tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_test(last_num, test_set, prob):\n",
    "\n",
    "    from itertools import product\n",
    "    col_n = len(prob.columns)\n",
    "    row_n = len(prob)\n",
    "    \n",
    "    \n",
    "    # best ensemble\n",
    "    best_f1 = 0\n",
    "    best_f1_t = 0\n",
    "    best_weights = None\n",
    "    best_weights_t = None\n",
    "    A = [i for i in range(last_num + 1)]\n",
    "    \n",
    "    for w in tqdm(product(A, repeat = col_n)):\n",
    "        \n",
    "        # 가중치 열 만들기\n",
    "        weight_frame = pd.DataFrame([w]*row_n, columns = prob.columns)\n",
    "        \n",
    "        # 가중치 곱해주기\n",
    "        prob_weight = prob * weight_frame\n",
    "        \n",
    "        # 가중치의 합\n",
    "        w_sum = sum(w)\n",
    "        \n",
    "        # 평균 계산해주기\n",
    "        final_proba = np.sum(prob_weight/w_sum, axis = 1)\n",
    "        \n",
    "        # 가중 평균 계산\n",
    "        y_pred = (final_proba > 0.5).astype(int)\n",
    "\n",
    "        # F1 스코어 계산\n",
    "        f1 = f1_score(test_set, y_pred)\n",
    "\n",
    "        # Threshold 스코어 계산\n",
    "        precision, recall, thresholds = precision_recall_curve(test_set, final_proba)\n",
    "        f1_scores = 2*recall*precision / (recall + precision)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        y_pred_custom_threshold = (final_proba >= best_threshold).astype(int)\n",
    "        f1_t = f1_score(test_set, y_pred_custom_threshold)\n",
    "\n",
    "        # 최고 성능 저장\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_weights = w\n",
    "\n",
    "        if f1_t > best_f1_t:\n",
    "            best_f1_t = f1_t\n",
    "            best_weights_t = w\n",
    "            \n",
    "#         print(f'weight: {w}, best_f1: {best_f1}, best_f1_t: {best_f1_t}, best_weights: {best_weights}, best_weights_t: {best_weights_t}')\n",
    "\n",
    "    print('종료되었습니다.')\n",
    "    return best_f1, best_f1_t, best_weights, best_weights_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad286e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용\n",
    "best_f1, best_f1_t, best_weights, best_weights_t = weight_test(last_num = 20, test_set = y_valid_xgb_last, prob = pd.DataFrame({'cat_proba': cat_proba, 'lgbm_proba': lgbm_proba, 'xgb_proba': xgb_proba}))\n",
    "print(f'best_f1: {best_f1}, best_f1_t: {best_f1_t}, best_weights: {best_weights}, best_weights_t: {best_weights_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a741a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best ensemble 적용\n",
    "cat = cat_proba * best_weights_t[0]\n",
    "lgbm = lgbm_proba * best_weights_t[1]\n",
    "xgb = xgb_proba * best_weights_t[2]\n",
    "y_best = (cat + lgbm + xgb)/(sum(best_weights_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_valid_xgb_last, y_best)\n",
    "f1_scores = 2*recall*precision / (recall + precision)\n",
    "weights_best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_custom_threshold = (y_best >= weights_best_threshold).astype(int)\n",
    "get_clf_eval(y_valid_xgb_last, y_pred_custom_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f418f",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a2009",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e973e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_cat = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_cat, y_cat):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    cat_best_params[\"random_seed\"] = 42\n",
    "    cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    cat_best_model.fit(\n",
    "        X_cat.iloc[train_idx], y_cat.iloc[train_idx],\n",
    "        eval_set = [(X_cat.iloc[valid_idx], y_cat.iloc[valid_idx])],\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose = 100, cat_features=cat_features_indices\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_cat.append(cat_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_cat):\n",
    "    pred_list.append(model.predict_proba(X_test_cat[X_test_cat.columns.difference(['inconsistant'])])[:, 1])\n",
    "\n",
    "# 확률값 평균내기\n",
    "cat_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(cat_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aad347",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183dbfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_lgbm = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_lgbm, y_lgbm):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    lgbm_best_params[\"random_seed\"] = 42\n",
    "    lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "\n",
    "    # fit the model\n",
    "    lgbm_best_model.fit(\n",
    "        X_lgbm.iloc[train_idx], y_lgbm.iloc[train_idx],\n",
    "        eval_set = [(X_lgbm.iloc[valid_idx], y_lgbm.iloc[valid_idx])],\n",
    "        categorical_feature=cat_features_indices,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_lgbm.append(lgbm_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52158908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_lgbm):\n",
    "    pred_list.append(model.predict_proba(X_test_lgbm[X_test_lgbm.columns.difference(['inconsistant'])])[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "lgbm_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(lgbm_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fde2d6",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d509129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 해당 모델 저장 리스트\n",
    "models_xgb = []\n",
    "\n",
    "# split마다 모델 적합하기\n",
    "for train_idx, valid_idx in tscv.split(X_xgb, y_xgb):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    xgb_best_params[\"seed\"] = 42\n",
    "    xgb_best_model = XGBClassifier(**xgb_best_params, early_stopping_rounds = 50)\n",
    "\n",
    "    # fit the model\n",
    "    xgb_best_model.fit(\n",
    "        X_xgb.iloc[train_idx], y_xgb.iloc[train_idx],\n",
    "        eval_set = [(X_xgb.iloc[valid_idx], y_xgb.iloc[valid_idx])],\n",
    "        verbose = 100\n",
    "    )\n",
    "\n",
    "    # 모델 결과 저장하기\n",
    "    models_xgb.append(xgb_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_list = []\n",
    "\n",
    "# 각 모델별 예측값 가져오기\n",
    "for i, model in enumerate(models_xgb):\n",
    "    pred_list.append(model.predict_proba(X_test_xgb[X_test_xgb.columns.difference(['inconsistant'])])[:, 1])\n",
    "    \n",
    "# 확률값 평균내기\n",
    "xgb_proba = np.mean(pred_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.where(xgb_proba >= 0.5, 1, 0), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "cat = cat_proba * best_weights_t[0]\n",
    "lgbm = lgbm_proba * best_weights_t[1]\n",
    "xgb = xgb_proba * best_weights_t[2]\n",
    "p = (cat + lgbm + xgb)/(sum(best_weights_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1403d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_equip2 = np.where(p >= weights_best_threshold, 'AbNormal', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830b83e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(p2_equip2, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ace53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_equip2 = np.where(p >= 0.5, 'AbNormal', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(p3_equip2, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 대입\n",
    "equip1_test['target'] = p2_equip1\n",
    "equip2_test['target'] = p2_equip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합체 및 정렬\n",
    "t = pd.concat([equip1_test.reset_index(), equip2_test.reset_index()]).sort_values('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a60d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 넣어주자\n",
    "t.index = t['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee47dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['target'] = np.where(df_test['inconsistant'] == 1, 'AbNormal', t['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e237d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(t['target'], df_test['inconsistant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5b99d",
   "metadata": {},
   "source": [
    "## 7. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = t['target']\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c717cfe",
   "metadata": {},
   "source": [
    "# 꼭 df_sub['target'] 확인하고 제출하시오"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
